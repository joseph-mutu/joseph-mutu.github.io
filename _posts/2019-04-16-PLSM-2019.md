---
layout:     post   				    # 使用的布局（不需要改）
title:      PLSM 				# 标题 
subtitle:   PLSM, Extension of PLSA, EM #副标题
date:       2019-04-17 				# 时间
author:     WYX 						# 作者
header-img: img/3.4.png 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - PGM,PLSM,EM
---

# PLSM

This blog is to analyze the paper"A Sequential Topic Model for Mining Recurrent
Activities from Video and Audio Data Logs". Please check the reference for more information. The main goal is to derive the results.

---



Observed data:



$$
X = (w,ta,d)
$$



Complete data:



$$
(X,Z) = (w,ta,d,z,ts)
$$



生成过程文字描述如下

![](https://ww1.sinaimg.cn/large/007i4MEmgy1g26okbawdfj31n80rmk56.jpg)

其概率图为

![](https://0d077ef9e74d8.cdn.sohucs.com/rnXSRfI_png)



Based on the graphical model, it can be expressed as:


$$
p(w,ta,d,z,ts) = \sum_z \sum_{ts}p(w,d,ta,d,z,ts) \\
\space\\
=\sum_z \sum_{ts}p(d)p(z,ts|d)p(w,tr|z) \\
\space\\
= \sum_z \sum_{ts}p(d)p(z|d)p(ts|z,d)p(w,ta-ts|z) \tag{1}
$$


the likelihood function of the corpus can be represented as:


$$
L(W,Ta,D|\theta) = \prod_{w=1}^{N_d}\prod_{ta=1}^{N_{Ta}}\prod_{d=1}^{N_d}P(w,ta,d)^{n(w,ta,d)} \tag{2}
$$


Take the log likelihood function:


$$
{\cal L(W,Ta,D|\theta)} = log L(W,Ta,D|\theta)\\
\space\\
 = n(w,ta,d)log\prod_{i=1}^{N_d}\prod_{j=1}^{N_{Ta}}\prod_{k=1}^{N_d}P(w_i,ta_j,d_k) \\
 \space\\
 =n(w,ta,d)\sum_{i=1}^{N_d}\sum_{j=1}^{N_{Ta}}\sum_{k=1}^{N_d} logP(w_i,ta_j,d_k) \\
 \space\\
 Take\space (1) \space into\space the\space formula \\
 \space\\
 =\sum_{i=1}^{N_d}\sum_{j=1}^{N_{Ta}}\sum_{k=1}^{N_d}n(w,ta,d) \underbrace{log\sum_z^{N_z} \sum_{ts}^{N_{ts}}p(d)p(z|d)p(ts|z,d)p(w,ta-ts|z)}_{hard \space to\space solve} \tag{3}
$$



## Introduce KL Divergence

The formula of KL divergence is:
$$
\mathrm{KL}(\mathrm{f}(\mathrm{x}) \| \mathrm{g}(\mathrm{x}))=\sum_{x \in X} f(x) * \log _{2} \frac{f(x)}{g(x)}
$$
In the paper, the real distribution is considered as the uniform distribution$U(0,Ts)$ whose probability density function is as follows:


$$
p(x) = \frac{1}{Ts-0} =  \frac{1}{Ts}
$$


By maximizing the gap between the wanted distribution and the uniform distribution at M-step, the sparse solution is encouraged. 



(3) now can be expressed as follows by adding the KL divergence:


$$
{\cal L(W,Ta,D|\theta)} =\sum_{i=1}^{N_d}\sum_{j=1}^{N_{Ta}}\sum_{k=1}^{N_d} n(w,ta,d)\underbrace{log\sum_z^{N_z} \sum_{ts}^{N_{ts}}p(d)p(z|d)p(ts|z,d)p(w,ta-ts|z)}_{hard \space to\space solve} \\-\sum_{t_{s}, z, d} \frac{\lambda_{z, d}}{T_{d s}} \cdot \log \left(p\left(t_{s} | z, d\right)\right) \tag{4}
$$

---

### Analysis of constant term in KL divergence

Ignore the weight parameter $\lambda_{z,d}$, the KL divergence should be



$$
\sum_{t_s,z,d} \frac{1}{T_{ds} - 0} \cdot log(\frac{\frac{1}{T_{ds} - 0}}{(p(t_s|z,d)})
$$



Because we will maximize the target function, so the constant in the KL Divergence can be ignored.


$$
\sum_{t_s,z,d} \frac{1}{T_{ds} - 0} \cdot log(\frac{\frac{1}{T_{ds} - 0}}{(p(t_s|z,d)})\\
\space \\
= \sum_{t_s,z,d} \frac{1}{T_{ds} - 0} \cdot [\underbrace {log({\frac{1}{T_{ds} - 0}})}_{constant} - log(p(t_s|z,d))]\\
$$





$$
= -\sum_{t_s,z,d} \frac{1}{T_{ds}}[log(p(t_s|z,d))]
$$

---



## EM Algo

$$
{\cal L(W,Ta,D|\theta)} =\sum_{i=1}^{N_d}\sum_{j=1}^{N_{Ta}}\sum_{k=1}^{N_d} n(w,ta,d)\underbrace{log\sum_z^{N_z} \sum_{ts}^{N_{ts}}p(d)p(z|d)p(ts|z,d)p(w,ta-ts|z)}_{hard \space to\space solve} \\-\sum_{t_{s}, z, d} \frac{\lambda_{z, d}}{T_{d s}} \cdot \log \left(p\left(t_s|z,d\right)\right) \tag{4}
$$



Because the above formula is hard to solve by directly using maximal likelihood:


$$
\frac{\partial log(f_1+f_2+f_3+...+f_n)}{\partial f_1}
$$


So apply EM algorithm to (4) by introducing the hidden variable distribution


$$
\sum_{i=1}^{N_d}\sum_{j=1}^{N_{Ta}}\sum_{k=1}^{N_d} n(w,ta,d)log\sum_z^{N_z} \sum_{ts}^{N_{ts}}p(d)p(z|d)p(ts|z,d)p(w,ta-ts|z) \\ 
\space\\
=\sum_{i=1}^{N_d}\sum_{j=1}^{N_{Ta}}\sum_{k=1}^{N_d} n(w,ta,d) \underbrace{log\sum_z^{N_z} \sum_{ts}^{N_{ts}} p(z,ts|w,ta,d)\frac{ p(d)p(z|d)p(ts|z,d)p(w,ta-ts|z)}{p(z,ts|w,ta,d)}}_{{f(\mathbb Ex)}} \geq \\
\space\\
=\sum_{i=1}^{N_d}\sum_{j=1}^{N_{Ta}}\sum_{k=1}^{N_d} n(w,ta,d) \underbrace{\sum_z^{N_z} \sum_{ts}^{N_{ts}}p(z,ts|w,ta,d) log\space[\frac{ p(d)p(z|d)p(ts|z,d)p(w,ta-ts|z)}{p(z,ts|w,ta,d)}]}_{\mathbb E[{f(x)}]} \tag{5}
$$


Split the fraction of the log:


$$
\sum_{i=1}^{N_d}\sum_{j=1}^{N_{Ta}}\sum_{k=1}^{N_d} n(w,ta,d) \sum_z^{N_z} \sum_{ts}^{N_{ts}}p(z,ts|w,ta,d) log\space[\space \underbrace{p(d)p(z|d)p(ts|z,d)p(w,ta-ts|z)}]\\ - \underbrace{\sum_{i=1}^{N_d}\sum_{j=1}^{N_{Ta}}\sum_{k=1}^{N_d} n(w,ta,d) \sum_z^{N_z} \sum_{ts}^{N_{ts}} log \space p(z,ts|w,ta,d)\space p(z,ts|w,ta,d)}_{constant}
$$


Again, because of the maximal step of EM algorithm, so ignore the constant term to simplify the formula. 



So, add the KL divergence. The Q function can be expressed as:


$$
E[{\cal L}] = \sum_{i=1}^{N_d}\sum_{j=1}^{N_{Ta}}\sum_{k=1}^{N_d} n(w,ta,d) \sum_z^{N_z} \sum_{ts}^{N_{ts}}p(z,ts|w,ta,d) log\space[ p(d)p(z|d)p(ts|z,d)p(w,ta-ts|z)]\\-\sum_{t_s, z,d} \frac{\lambda_{z, d}}{T_{d s}} \cdot \log \left(p\left(t_s|z,d\right)\right)  \tag{6}
$$




### E step



The E-step is to calculate the posterior distribution of the hidden variables


$$
p(z,ts|w,ta,d) =  \frac{p(z,ts,w,ta,d)}{\sum_{z=1}^{N_{z}} \sum_{ts=1}^{T_{d s}}p(z,ts,w,ta,d)} = \frac{p(z,ts,w,ta,d)}{p(w,ta,d)}
$$



**After E-step, the posterior distribution is the constant**



### M-step



M step is to maximize the parameters with constraints



$$
\theta^{(t+1)} = arg\max_\theta \int_{Z,TS} P(Z,TS|\theta^{(t)}) logP(W,Ta,D,Z,TS|\theta) dzdts
$$



Build the Lagrange function, the original function $E[L]$ has  following constraints for each document



$$
\sum_{j=1}^{N_w} \sum_{i=1}^{N_{tr}} p(w_j,tr_i|z) = 1 \\
\sum_{k=1}^{N_z} \sum_{i=1}^{N_{ts}} p(z_k,ts_i|d) = 1
$$





#### Build Lagrange function



**Lagrange function** can  be expressed as follows:


$$
Lag = E[\mathcal{L}] + \sum_{d=1}^{N_d}\lambda_d (1 -\sum_{j=1}^{N_w} \sum_{i=1}^{N_{tr}} p(w_j,tr_i|z) ) +\\ \sum_{d=1}^{N_d}\mu_d(1-\sum_{k=1}^{N_z} \sum_{i=1}^{N_{ts}} p(z_k,ts_i|d)) \tag{7}
$$

$$
= \sum_{d=1}^{N_d}\sum_{ta=1}^{N_{Ta}}\sum_{w=1}^{N_w} n(w,ta,d) \sum_z^{N_z} \sum_{ts}^{N_{ts}} p(z,ts|w,ta,d) log\space[p(d)p(z|d)p(ts|z,d)p(w,tr|z)]\\-\sum_{t_s, z,d} \frac{\lambda_{z, d}}{T_{d s}} \cdot \log \left(p\left(t_s|z,d\right)\right)  \\
+ \sum_{d=1}^{N_d}\lambda_d (1 -\sum_{w=1}^{N_w} \sum_{tr=1}^{N_{tr}} p(w,tr|z) )  +\\ \sum_{d=1}^{N_d}\mu_d(1-\sum_{z=1}^{N_z} \sum_{ts=1}^{N_{ts}} p(z,ts|d))
$$




$$
= \sum_{d=1}^{N_d}\sum_{tr=0}^{Tz-1}\sum_{w=1}^{N_w}\sum_{z=1}^{N_z} \sum_{ts=1}^{N_{ts}} n(w,ts+tr,d)  p(z,ts|w,ts+tr,d) log\space[p(d)p(z|d)p(ts|z,d)p(w,tr|z)]\\-\sum_{t_s, z,d} \frac{\lambda_{z, d}}{T_{d s}}\log(p(t_s|z,d))  \\
+ \sum_{d=1}^{N_d}\lambda_d (1 -\sum_{w=1}^{N_w} \sum_{tr=1}^{N_{tr}} p(w,tr|z) )  +\\ \sum_{d=1}^{N_d}\mu_d(1-\sum_{z=1}^{N_z} \sum_{ts=1}^{N_{ts}} p(z,ts|d))
$$


The other three sum symbols $\sum_{tr=0}^{Tz-1}\sum_{w=1}^{N_w}\sum_{z=1}^{N_z}$can be eliminated when we focus one parameter. The KL divergence can be ignored, has no relation for current parameter


$$
\frac{\partial {\cal{Lag}}}{\partial p(w,tr|z)} \Rightarrow \frac{\sum_{d=1}^{N_d}\sum_{ts=1}^{N_{ts}}n(w,ts+tr,d)  p(z,ts|w,ts+tr,d)}{p(w,tr|z)} + \sum_{d=1}^{N_d}\lambda_d = 0\\
\space\\

\Rightarrow \sum_{d=1}^{N_d}\sum_{ts=1}^{N_{ts}}n(w,ts+tr,d)p(z,ts|w,ts+tr,d) = p(w,tr|z)\sum_{d=1}^{N_d}\lambda_d\\
\space\\
Introduce \space the \space contraint
\space\\
\Rightarrow \sum_{w=1}^{N_w} \sum_{tr=1}^{N_{tr}}\sum_{d=1}^{N_d}\sum_{ts=1}^{N_{ts}}n(w,ts+tr,d)p(z,ts|w,ts+tr,d) = \underbrace{\sum_{w=1}^{N_w} \sum_{tr=1}^{N_{tr}}p(w,tr|z)}_1\sum_{d=1}^{N_d}\lambda_d\\
\space\\
\Rightarrow \sum_{d=1}^{N_d}\lambda_d =  \sum_{w=1}^{N_w} \sum_{tr=1}^{N_{tr}}\sum_{d=1}^{N_d}\sum_{ts=1}^{N_{ts}}n(w,ts+tr,d)p(z,ts|w,ts+tr,d)
$$

$$
Take \space \sum_{d=1}^{N_d}\lambda_d \space back \space in \space to \space the \space formula \\
\space \\
\Rightarrow \sum_{d=1}^{N_d}\sum_{ts=1}^{N_{ts}}n(w,ts+tr,d)p(z,ts|w,ts+tr,d) = p(w,tr|z)\sum_{d=1}^{N_d}\lambda_d\\
\space \\
\Rightarrow p(w,tr|z) = \frac{\sum_{d=1}^{N_d}\sum_{ts=1}^{N_{ts}}n(w,ts+tr,d)p(z,ts|w,ts+tr,d)}{\sum_{d=1}^{N_d}\lambda_d}\\
\space \\
\Rightarrow p(w,tr|z) = \frac{\sum_{d=1}^{N_d}\sum_{ts=1}^{N_{ts}}n(w,ts+tr,d)p(z,ts|w,ts+tr,d)}{\sum_{w=1}^{N_w} \sum_{tr=1}^{N_{tr}}\sum_{d=1}^{N_d}\sum_{ts=1}^{N_{ts}}n(w,ts+tr,d)p(z,ts|w,ts+tr,d)}
$$






---





$$
E[\mathcal{L}] =\sum_{d=1}^{D} \sum_{w=1}^{N_w} \sum_{t_a=1}^{T_d}\sum_{z=1}^{N_z}\sum_{t_s=1}^{T_{ds}} n(w,t_a,d) p(z,t_s|w,t_a,d) [log(p(d)) + log(p(t_s,z|d))\\ + log(w,ta-ts|z)]
$$

$$
=\sum_{d=1}^{D} \sum_{w=1}^{N_w} \sum_{t_a=1}^{T_d}\sum_{z=1}^{N_z}\sum_{t_s=1}^{T_{ds}} \underbrace{n(w,t_a,d)}_{与z,ts无关} \underbrace{p(z,t_s|w,t_a,d)}_{提取分析}\sum_{z=1}^{N_z}\sum_{t_s=1}^{T_{ds}}[log(p(d)) + log(p(t_s,z|d))\\ + log(w,ta-ts|z)]
$$

$$
\sum_{z=1}^{N_z}\sum_{t_s=1}^{T_{ds}} p\left(z, t_{s} | w, t_{a}, d\right)=\sum_{z=1}^{N_z}\sum_{t_s=1}^{T_{ds}} \frac{p\left(w, t_{a}, d, z, t_{s}\right)}{\sum_{z=1}^{N_{z}} \sum_{t_{s}=1}^{T_{d s}}p\left(w, t_{a}, d\right)}= 1
$$

$$
\Rightarrow \sum_{d=1}^{D} \sum_{w=1}^{N_w} \sum_{t_a=1}^{T_d}n(w,t_a,d) p(z,t_s|w,t_a,d)\sum_{z=1}^{N_z}\sum_{t_s=1}^{T_{ds}}[log(p(d)) + log(p(t_s,z|d))\\ + log(w,ta-ts|z)]
$$


添加拉格朗日乘数，进行求解


$$
\Rightarrow \sum_{d=1}^{D} \sum_{w=1}^{N_w} \sum_{t_a=1}^{T_d}n(w,t_a,d) p(z,t_s|w,t_a,d)\sum_{z=1}^{N_z}\sum_{t_s=1}^{T_{ds}}[log(p(d)) + log(p(t_s,z|d))\\ + log(w,ta-ts|z)]+ \sum_D\lambda_{D1} (1 - \sum_{w=1}^{N_w} \sum_{t_a=1}^{T_d} \sum_{t_s=1}^{T_{ds}}p(w_j,t_a-t_s|z) )
$$











**Solve $p(w,t_r|z)$**

Because we want to derive $p(w,t_r|z)$, $t_r$ is decided by $t_a-t_s$,and this probability is based on given topic z. 
$$
\frac{\partial Lag}{p(w,t_a-ts|z)} =\frac{\sum_{d=1}^{D} \sum_{ta=1}^{T_d} \sum_{ts=1}^{T_{ds}} \left(w, t_a-t_s, d\right) p\left(z, t_{s} | w,  t_a-t_s, d\right)}{p(w, t_a-t_s|z)} -\lambda_1
$$


Because
$$
\sum_{k=1}^{N_z}\sum_{i=1}^{N_{ts}}p(w_k,ts_i|z_k,d) = 1 \tag{2}
$$
##### **$p(z|d)$** can be solved from $\lambda_1$ 

$$
\frac{\partial Lag}{p(z_k|d)} =\frac{\sum_{d=1}^{D} \sum_{ta=1}^{T_d} \sum_{ts=1}^{T_{ds}} \left(w, t_{s}+t_{r}, d\right) p\left(z, t_{s} | w, t_{s}+t_{r}, d\right)}{p(w,t_r|z)} -\lambda_1
$$

$$
\Longrightarrow \sum_{d=1}^{D} \sum_{ta=1}^{T_d} \sum_{ts=1}^{T_{ds}} \left(w, t_{s}+t_{r}, d\right) p\left(z, t_{s} | w, t_{s}+t_{r}, d\right) = \lambda_1 p(w,t_r|z)
$$

$$
\Longrightarrow \sum_{k=1}^{N_z}\sum_{i=1}^{N_{ts}} \sum_{d=1}^{D} \sum_{ta=1}^{T_d} \sum_{ts=1}^{T_ds} \left(w, t_{s}+t_{r}, d\right) p\left(z, t_{s} | w, t_{s}+t_{r}, d\right)= \sum_{k=1}^{N_z}\sum_{i=1}^{N_{ts}} \lambda_1 p(w,t_r|z)
$$

$$
\Longrightarrow \sum_{k=1}^{N_z}\sum_{t_{s}=1}^{T_{d s}} \sum_{t_{r}=0}^{T_{z}-1} \sum_{w=1}^{N_{w}} n\left(w, t_{s}+t_{r}, d\right) p\left(z, t_{s} | w, t_{s}+t_{r}, d\right) = \lambda_1 \sum_{k=1}^{N_z} p(z_k|d)
$$

$$
\Longrightarrow \sum_{k=1}^{N_z}\sum_{t_{s}=1}^{T_{d s}} \sum_{t_{r}=0}^{T_{z}-1} \sum_{w=1}^{N_{w}} n\left(w, t_{s}+t_{r}, d\right) p\left(z, t_{s} | w, t_{s}+t_{r}, d\right) = \lambda_1 \\
$$



Take $\lambda_1$ Back , we have 
$$
\sum_{t_{s}=1}^{T_{d s}} \sum_{t_{r}=0}^{T_{z}-1} \sum_{w=1}^{N_{w}} n\left(w, t_{s}+t_{r}, d\right) p\left(z, t_{s} | w, t_{s}+t_{r}, d\right) = \lambda_1 p(z_k|d) \\
\Longrightarrow p(z_k|d) = \frac{\sum_{t_{s}=1}^{T_{d s}} \sum_{t_{r}=0}^{T_{z}-1} \sum_{w=1}^{N_{w}} n\left(w, t_{s}+t_{r}, d\right) p\left(z, t_{s} | w, t_{s}+t_{r}, d\right)}{\sum_{k=1}^{N_z}\sum_{t_{s}=1}^{T_{d s}} \sum_{t_{r}=0}^{T_{z}-1} \sum_{w=1}^{N_{w}} n\left(w, t_{s}+t_{r}, d\right) p\left(z, t_{s} | w, t_{s}+t_{r}, d\right) }
$$



**Note**: After E-step, $p(z,ts|w,t_s,ta,d)$ is a constant



Because of $\sum_{k=1}^{N_z} and \sum_{t_s}^{T_{ds}}$ , $p(z,t_s|w,t_s + t_r,d)$ is 1
$$
p\left(z_k, t_{s} | w, t_{a}, d\right)=\frac{p\left(w, t_{a}, d, z_k, t_{s}\right)}{\sum_{z}^{N_z} \sum_{t_s}^{T_{ds}} p\left(w, t_{a}, d\right)}
$$
Results mentioned in the paper
$$
p(z | d) \propto \sum_{t_{s}=1}^{T_{d s}} \sum_{t_{r}=0}^{T_{z}-1} \sum_{w=1}^{N_{w}} n\left(w, t_{s}+t_{r}, d\right) p\left(z, t_{s} | w, t_{s}+t_{r}, d\right)
$$
