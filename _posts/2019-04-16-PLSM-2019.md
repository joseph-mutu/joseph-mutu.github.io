---
layout:     post   				# 使用的布局（不需要改）
title:      PLSA 				# 标题 
subtitle:   PLSA, EM 				#副标题
date:       2019-04-16 				# 时间
author:     WYX 				# 作者
header-img: img/3.4.png 		#这篇文章标题背景图片
catalog: true 					# 是否归档
tags:						#标签
    - PGM,PLSA,EM

---

# PLSM

This blog is to analyze the paper"A Sequential Topic Model for Mining Recurrent
Activities from Video and Audio Data Logs". Please check the reference for more information. The main goal is to derive the results.

---



Observed data:



$$
X = (w,ta,d)
$$



Complete data:



$$
(X,Z) = (w,ta,d,z,ts)
$$



The hidden variables in PLSM model is z and ts.



The generative process can be expressed as follow:

![](https://ww1.sinaimg.cn/large/007i4MEmgy1g26okbawdfj31n80rmk56.jpg)



The graphical expression can be shown as:

![](https://ww1.sinaimg.cn/large/007i4MEmgy1g28zi3tej0j315i0lm41h.jpg)

in which,

- n(w,ta,d) represents how many time the word (w,ta,d) appears 

- ta  = ts + tr 

- First draw the document p(d). 

- Then, one square at z_ts tables is drawn meaning that the topic z and the starting time ts is decided. It represents two steps. p(z \|d) and p(ts \|z,d). The reason why we could do two drawn at one step is because:
  
  
  $$
p(z,ts|d) = p(z|d)p(ts|z,d)
  $$





- After the topic z is decided, draw p(w,tr \|z=1,2,3,....,) based on the topic decided above. (drawing p(w,tr\|z), in physical meaning, means select one square at p(w,tr\|z) based on their probabilities.)

- based on p(z,ts\|d) and p(w,tr\|z), the word position can be defined at w_ta table.

It should be noticed that:

- z_ts is a probability table, which means that the number at each square of z_ts should sum to 1. And the number means the probability.
- each p(w,tr\|z) table should also sum to 1. the number of p(w,tr|z) table depends on how many topics you have.
- in general case, we have 1+number of topic constraints.



its corresponding probability graphical model is:



![](https://0d077ef9e74d8.cdn.sohucs.com/rnXSRfI_png)



Based on the graphical model, the generative process can be formulated as:



$$
p(w,ta,d) = \sum_z \sum_{ts}p(w,d,ta,d,z,ts) \\
\space\\
=\sum_z \sum_{ts}p(d)p(z,ts|d)p(w,tr|z) \\
\space\\
= \sum_z \sum_{ts}p(d)p(z|d)p(ts|z,d)p(w,ta-ts|z) \tag{1}
$$



So, the likelihood function of the corpus can be represented as:



$$
L(W,Ta,D|\theta) = \prod_{w=1}^{N_d}\prod_{ta=1}^{N_{Ta}}\prod_{d=1}^{N_d}P(w,ta,d)^{n(w,ta,d)} \tag{2}
$$



Take the log likelihood function:



$$
{\cal L(W,Ta,D|\theta)} = log L(W,Ta,D|\theta)\\
\space\\
 = n(w,ta,d)log\prod_{i=1}^{N_d}\prod_{j=1}^{N_{Ta}}\prod_{k=1}^{N_d}P(w_i,ta_j,d_k) \\
 \space\\
 =n(w,ta,d)\sum_{i=1}^{N_d}\sum_{j=1}^{N_{Ta}}\sum_{k=1}^{N_d} logP(w_i,ta_j,d_k) \\
 \space\\
 Take\space (1) \space into\space the\space formula \\
 \space\\
 =\sum_{i=1}^{N_d}\sum_{j=1}^{N_{Ta}}\sum_{k=1}^{N_d}n(w,ta,d) \underbrace{log\sum_z^{N_z} \sum_{ts}^{N_{ts}}p(d)p(z|d)p(ts|z,d)p(w,ta-ts|z)}_{hard \space to\space solve} \tag{3}
$$



## Introduce KL Divergence

The formula of KL divergence is:



$$
\mathrm{KL}(\mathrm{f}(\mathrm{x}) \| \mathrm{g}(\mathrm{x}))=\sum_{x \in X} f(x) * \log _{2} \frac{f(x)}{g(x)}
$$



In the paper, the real distribution is considered as the uniform distribution $U(0,Ts)$ whose probability density function is as follows:



$$
p(x) = \frac{1}{Ts-0} =  \frac{1}{Ts}
$$



By maximizing the gap between the wanted distribution and the uniform distribution at M-step, the sparse solution is encouraged. 



(3) now can be expressed as follows by adding the KL divergence:



$$
{\cal L(W,Ta,D|\theta)} =\sum_{i=1}^{N_d}\sum_{j=1}^{N_{Ta}}\sum_{k=1}^{N_d} n(w,ta,d)\underbrace{log\sum_z^{N_z} \sum_{ts}^{N_{ts}}p(d)p(z|d)p(ts|z,d)p(w,ta-ts|z)}_{hard \space to\space solve} \\-\sum_{t_{s}, z, d} \frac{\lambda_{z, d}}{T_{d s}} \cdot \log \left(p\left(t_{s} | z, d\right)\right) \tag{4}
$$

---



### Analysis of the constant term in KL divergence

Ignore the weight parameter $\lambda_{z,d}$, the KL divergence should be



$$
\sum_{t_s,z,d} \frac{1}{T_{ds} - 0} \cdot log(\frac{\frac{1}{T_{ds} - 0}}{(p(t_s|z,d)})
$$



Due to the maximal step of EM algorithm, the constant in the KL Divergence can be ignored.


$$
\sum_{t_s,z,d} \frac{1}{T_{ds} - 0} \cdot log(\frac{\frac{1}{T_{ds} - 0}}{(p(t_s|z,d)})\\
\space \\
= \sum_{t_s,z,d} \frac{1}{T_{ds} - 0} \cdot [\underbrace {log({\frac{1}{T_{ds} - 0}})}_{constant} - log(p(t_s|z,d))]\\
$$



$$
= -\sum_{t_s,z,d} \frac{1}{T_{ds}}[log(p(t_s|z,d))]
$$

---



## EM Algo

$$
{\cal L(W,Ta,D|\theta)} =\sum_{i=1}^{N_d}\sum_{j=1}^{N_{Ta}}\sum_{k=1}^{N_d} n(w,ta,d)\underbrace{log\sum_z^{N_z} \sum_{ts}^{N_{ts}}p(d)p(z|d)p(ts|z,d)p(w,ta-ts|z)}_{hard \space to\space solve} \\-\sum_{t_{s}, z, d} \frac{\lambda_{z, d}}{T_{d s}} \cdot \log \left(p\left(t_s|z,d\right)\right) \tag{4}
$$



Because the above formula is hard to solve by directly using maximal likelihood:



$$
\frac{\partial log(f_1+f_2+f_3+...+f_n)}{\partial f_1}
$$



So apply EM algorithm to (4) by introducing the hidden variable distribution



$$
\sum_{i=1}^{N_d}\sum_{j=1}^{N_{Ta}}\sum_{k=1}^{N_d} n(w,ta,d)log\sum_z^{N_z} \sum_{ts}^{N_{ts}}p(d)p(z|d)p(ts|z,d)p(w,ta-ts|z) \\ 
\space\\
=\sum_{i=1}^{N_d}\sum_{j=1}^{N_{Ta}}\sum_{k=1}^{N_d} n(w,ta,d) \underbrace{log\sum_z^{N_z} \sum_{ts}^{N_{ts}} p(z,ts|w,ta,d)\frac{ p(d)p(z|d)p(ts|z,d)p(w,ta-ts|z)}{p(z,ts|w,ta,d)}}_{{f(\mathbb Ex)}} \geq \\
\space\\
=\underbrace{\sum_{i=1}^{N_d}\sum_{j=1}^{N_{Ta}}\sum_{k=1}^{N_d} n(w,ta,d) \underbrace{\sum_z^{N_z} \sum_{ts}^{N_{ts}}p(z,ts|w,ta,d) log\space[\frac{ p(d)p(z|d)p(ts|z,d)p(w,ta-ts|z)}{p(z,ts|w,ta,d)}]}_{\mathbb E[{f(x)}]}}_{ELBO} \tag{5}
$$



EM algorithm is to maximize the ELBO of the primal log likelihood function



Split the fraction of the log function:



$$
\sum_{i=1}^{N_d}\sum_{j=1}^{N_{Ta}}\sum_{k=1}^{N_d} n(w,ta,d) \sum_z^{N_z} \sum_{ts}^{N_{ts}}p(z,ts|w,ta,d) log\space[\space \underbrace{p(d)p(z|d)p(ts|z,d)p(w,ta-ts|z)}]\\ - \underbrace{\sum_{i=1}^{N_d}\sum_{j=1}^{N_{Ta}}\sum_{k=1}^{N_d} n(w,ta,d) \sum_z^{N_z} \sum_{ts}^{N_{ts}} log \space p(z,ts|w,ta,d)\space p(z,ts|w,ta,d)}_{constant}
$$



Due to the maximal step of EM algorithm, the constant term can be left out. 



After adding the KL divergence. The so-called Q function can be expressed as:



$$
E[{\cal L}] = \sum_{w=1}^{N_d}\sum_{ta=1}^{N_{Ta}}\sum_{d=1}^{N_d} n(w,ta,d) \sum_{z=1}^{N_z} \sum_{ts=1}^{N_{ts}}p(z,ts|w,ta,d) log\space[ p(d)p(z|d)p(ts|z,d)p(w,ta-ts|z)]\\-\sum_{t_s, z,d} \frac{\lambda_{z, d}}{T_{d s}} \cdot \log \left(p\left(t_s|z,d\right)\right)  \tag{6}
$$




### E step



The E-step is to calculate the posterior distribution of the hidden variables:



$$
p(z,ts|w,ta,d) =  \frac{p(z,ts,w,ta,d)}{\sum_{z=1}^{N_{z}} \sum_{ts=1}^{T_{d s}}p(z,ts,w,ta,d)} = \frac{p(z,ts,w,ta,d)}{p(w,ta,d)}
$$



**After E-step, the posterior distribution is the constant. The posterior distribution will be calculated by using the initialization of parameters, which is p(z,ts|d) and p(w,tr|z). In simple words, we need to initialize two tables shown at the graphical representation.**



### M-step



M step is to maximize the parameters with constraints



$$
\theta^{(t+1)} = arg\max_\theta \int_{Z,TS} P(Z,TS|\theta^{(t)}) logP(W,Ta,D,Z,TS|\theta) \space dZ \space dTS \\
\space\\
s.t \space \sum_{w=1}^{N_w} \sum_{tr=1}^{N_{tr}} p(w,tr|z) = 1 \\
\space\\
\space \space \space \space \space \space \sum_{z=1}^{N_z} \sum_{ts=1}^{N_{ts}} p(z,ts|d) = 1
$$





**Note: in the solution part, this blog tries to solve different parameters (p(w,tr|z),p(z,ts|d)) and (p(w,tr|z),p(z|d),p(ts|z,d))**

The difference between two is to consider p(z,ts|d) as the entirety or not


$$
p(z,ts|d) = p(z|d)p(ts|z,d)
$$


It should be noticed that when the parameter p(z,ts|d) is split into two parts, the constraints should also be changed.


$$
\sum_{z=1}^{N_z} \sum_{ts=1}^{N_{ts}} p(z,ts|d) = 1 \Rightarrow \begin{cases}
\sum_z p(z|d) &= 1 \\
\sum_{ts} p(ts|z,d) & = 1
\end{cases}
$$


which means that z_ts is not the single table anymore. There are two tables.





#### Build Lagrange function 1



For the first case, because we want the z_ts table to be sparse, so the wanted distribution is changed here:



$$
E[{\cal L}] = \sum_{w=1}^{N_d}\sum_{ta=1}^{N_{Ta}}\sum_{d=1}^{N_d} n(w,ta,d) \sum_{z=1}^{N_z} \sum_{ts=1}^{N_{ts}}p(z,ts|w,ta,d) log\space[ p(d)p(z|d)p(ts|z,d)p(w,ta-ts|z)]\\\underbrace{-\sum_{t_s, z,d} \frac{\lambda_{z, d}}{T_{ds}} \cdot \log(p(t_s|z,d))}_{change} \\
\space\\
\Rightarrow E[{\cal L}] = \sum_{w=1}^{N_d}\sum_{ta=1}^{N_{Ta}}\sum_{d=1}^{N_d} n(w,ta,d) \sum_{z=1}^{N_z} \sum_{ts=1}^{N_{ts}}p(z,ts|w,ta,d) log\space[ p(d)p(z|d)p(ts|z,d)p(w,ta-ts|z)]\\\underbrace{-\sum_{t_s, z,d} \frac{\lambda_{z, d}}{T_{ds}} \cdot \log(p(z,t_s|d))}_{joint\space distribution}
$$



**Lagrange function** can  be expressed as follows:



$$
Lag = E[\mathcal{L}] + \sum_{z=1}^{N_z}\lambda_z (1 -\sum_{w=1}^{N_w} \sum_{tr=1}^{N_{tr}} p(w,tr|z) ) +\\ \sum_{d=1}^{N_d}\mu_d(1-\sum_{z=1}^{N_z} \sum_{ts=1}^{N_{ts}} p(z,ts|d)) \tag{7}
$$

$$
= \sum_{d=1}^{N_d}\sum_{ta=1}^{N_{Ta}}\sum_{w=1}^{N_w} n(w,ta,d) \sum_z^{N_z} \sum_{ts}^{N_{ts}} p(z,ts|w,ta,d) log\space[p(d)p(z|d)p(ts|z,d)p(w,tr|z)]\\-\sum_{t_s, z,d} \frac{\lambda_{z, d}}{T_{d s}} \cdot \log \left(p\left(t_s|z,d\right)\right) 
+ \sum_{z=1}^{N_z}\lambda_z (1 -\sum_{w=1}^{N_w} \sum_{tr=1}^{N_{tr}} p(w,tr|z) )  +\\ \sum_{d=1}^{N_d}\mu_d(1-\sum_{z=1}^{N_z} \sum_{ts=1}^{N_{ts}} p(z,ts|d))
$$



Recall that


$$
ta = tr + ts \\
\space\\
Replace \space ta \space with\space the\space equation \space introducing \space tr \\
\space \\
do\space the\space same\space thing\space to\space the\space \sum_{ta}^{N_{Ta}}
$$



The range of tr can be checked at the paper.

> tr $\in$ [0,...,Tz-1]),
> where tr denotes the relative time at which a word occurs within a topic. p7/33



$$
= \sum_{d=1}^{N_d}\underbrace{\sum_{tr=0}^{Tz-1}}_{ta\rightarrow tr}\sum_{w=1}^{N_w}\sum_{z=1}^{N_z} \sum_{ts=1}^{N_{ts}} n(w,ts+tr,d)  p(z,ts|w,ts+tr,d) log\space[p(d)p(z|d)p(ts|z,d)p(w,tr|z)]\\-\sum_{t_s, z,d} \frac{\lambda_{z, d}}{T_{d s}}\log(p(z,t_s|d))  \\
+ \sum_{z=1}^{N_z}\lambda_z (1 -\sum_{w=1}^{N_w} \sum_{tr=1}^{N_{tr}} p(w,tr|z) )  +\\ \sum_{d=1}^{N_d}\mu_d(1-\sum_{z=1}^{N_z} \sum_{ts=1}^{N_{ts}} p(z,ts|d))
$$



Each p(w,tr|z) is one parameter. In physical meaning, it means each square at p(w,tr|z)[motif] is one parameter.

The other three sum symbols $\sum_{tr=0}^{Tz-1}\sum_{w=1}^{N_w}\sum_{z=1}^{N_z}$can be eliminated when we focus one parameter. The KL divergence can be ignored, has no relation for current parameter





#### Solve p(w,tr|z)




$$
\frac{\partial {\cal{Lag}}}{\partial p(w,tr|z)} \Rightarrow \frac{\sum_{d=1}^{N_d}\sum_{ts=1}^{N_{ts}}n(w,ts+tr,d)  p(z,ts|w,ts+tr,d)}{p(w,tr|z)} - \sum_{z=1}^{N_z}\lambda_z = 0\\
\space\\

\Rightarrow \sum_{d=1}^{N_d}\sum_{ts=1}^{N_{ts}}n(w,ts+tr,d)p(z,ts|w,ts+tr,d) = p(w,tr|z)\sum_{z=1}^{N_z}\lambda_z\\
\space\\
Introduce \space the \space contraint\\
\space\\
\Rightarrow \sum_{w=1}^{N_w} \sum_{tr=1}^{N_{tr}}\sum_{d=1}^{N_d}\sum_{ts=1}^{N_{ts}}n(w,ts+tr,d)p(z,ts|w,ts+tr,d) = \underbrace{\sum_{w=1}^{N_w} \sum_{tr=1}^{N_{tr}}p(w,tr|z)}_1\sum_{z=1}^{N_z}\lambda_z\\
\space\\
\Rightarrow \sum_{z=1}^{N_z}\lambda_z =  \sum_{w=1}^{N_w} \sum_{tr=1}^{N_{tr}}\sum_{d=1}^{N_d}\sum_{ts=1}^{N_{ts}}n(w,ts+tr,d)p(z,ts|w,ts+tr,d) \tag{8}
$$

$$
Take \space\sum_{z=1}^{N_z}\lambda_z \space back \space in \space to \space the \space formula \\
\space \\
\Rightarrow \sum_{d=1}^{N_d}\sum_{ts=1}^{N_{ts}}n(w,ts+tr,d)p(z,ts|w,ts+tr,d) = p(w,tr|z)\sum_{z=1}^{N_z}\lambda_z\\
\space \\
\Rightarrow p(w,tr|z) = \frac{\sum_{d=1}^{N_d}\sum_{ts=1}^{N_{ts}}n(w,ts+tr,d)p(z,ts|w,ts+tr,d)}{\sum_{z=1}^{N_z}\lambda_z}\\
\space \\
\Rightarrow p(w,tr|z) = \frac{\sum_{d=1}^{N_d}\sum_{ts=1}^{N_{ts}}n(w,ts+tr,d)p(z,ts|w,ts+tr,d)}{\sum_{w=1}^{N_w} \sum_{tr=1}^{N_{tr}}\sum_{d=1}^{N_d}\sum_{ts=1}^{N_{ts}}n(w,ts+tr,d)p(z,ts|w,ts+tr,d)}
$$



the result can be expressed as:


$$
\\
\space \\
\Rightarrow p(w,tr|z) \propto \sum_{d=1}^{N_d}\sum_{ts=1}^{N_{ts}}n(w,ts+tr,d)p(z,ts|w,ts+tr,d)
$$






#### p(z,ts|d)



Here is some difference with the paper. In the paper, it solves p(z|d) and p(ts|z,d) separately. And in this blog, it tries to solve p(z,ts|d) and also p(z|d), p(ts|z,d).

It should be noticed that, the constraints will differ due to the different definition of parameters. Also apply to KL divergence added.



First recall:


$$
p(z,ts|d) = p(z|d)p(ts|z,d)
$$



So Lagrange function can be written as:



$$
= \sum_{d=1}^{N_d}\sum_{tr=0}^{Tz-1}\sum_{w=1}^{N_w}\sum_{z=1}^{N_z} \sum_{ts=1}^{N_{ts}} n(w,ts+tr,d)  p(z,ts|w,ts+tr,d) log\space[p(d)\underbrace{p(z,ts|d)}_{p(z|d)p(ts|z,d)}p(w,tr|z)]\\-\sum_{t_s, z,d} \frac{\lambda_{z, d}}{T_{d s}}\log(p(z,t_s|d))  \\
+ \sum_{z=1}^{N_z}\lambda_z (1 -\sum_{w=1}^{N_w} \sum_{tr=1}^{N_{tr}} p(w,tr|z) )  \\+ \sum_{d=1}^{N_d}\mu_d(1-\sum_{z=1}^{N_z} \sum_{ts=1}^{N_{ts}} p(z,ts|d))
$$


p(z,ts|d) represents one parameter, so it eliminates three sums $\sum_{d=1}^{N_d}\sum_{z=1}^{N_z} \sum_{ts=1}^{N_{ts}}$



$$
\frac{\partial {\cal{Lag}}}{\partial p(z,ts|d)} \Rightarrow \frac{\sum_{tr=0}^{Tz-1}\sum_{w=1}^{N_w}n(w,ts+tr,d)  p(z,ts|w,ts+tr,d) }{\underbrace{p(z,ts|d)}} \\
\space \\
- \frac{\lambda_{z,d}}{T_{ds}\underbrace{p(z,ts|d)}} - \sum_{d=1}^{N_d}\mu_d = 0 \tag{9}
$$



$$
\Rightarrow \frac{1}{p(z,ts|d)}(\sum_{tr=0}^{Tz-1}\sum_{w=1}^{N_w} n(w,ts+tr,d)  p(z,ts|w,ts+tr,d)- \frac{\lambda_{z,d}}{ T_{ds} } ) = \sum_{d=1}^{N_d}\mu_d \\
\space\\
\Rightarrow \sum_{tr=0}^{Tz-1}\sum_{w=1}^{N_w} n(w,ts+tr,d)  p(z,ts|w,ts+tr,d)- \frac{\lambda_{z,d}}{ T_{ds} }  = p(z,ts|d)\sum_{d=1}^{N_d}\mu_d\\
\space\\
Introduce \space the \space contraint \space \sum_{z=1}^{N_z} \sum_{ts=1}^{N_{ts}} p(z,ts|d) = 1\\
\space\\
\Rightarrow \underbrace{\sum_{z=1}^{N_z} \sum_{ts=1}^{N_{ts}}} \sum_{tr=0}^{Tz-1}\sum_{w=1}^{N_w}n(w,ts+tr,d)  p(z,ts|w,ts+tr,d)- \frac{\lambda_{z,d}}{ T_{ds} }  = \sum_{z=1}^{N_z} \sum_{ts=1}^{N_{ts}} p(z,ts|d)\underbrace{\sum_{d=1}^{N_d}\mu_d}\\
\space\\
\Rightarrow \sum_{z=1}^{N_z} \sum_{ts=1}^{N_{ts}}\sum_{tr=0}^{Tz-1}\sum_{w=1}^{N_w} n(w,ts+tr,d)  p(z,ts|w,ts+tr,d)- \frac{\lambda_{z,d}}{ T_{ds} }  = \underbrace{\sum_{d=1}^{N_d}\mu_d} \underbrace{\sum_{z=1}^{N_z} \sum_{ts=1}^{N_{ts}} p(z,ts|d)}_1\\
\space\\
\Rightarrow \sum_{d=1}^{N_d}\mu_d = \sum_{z=1}^{N_z} \sum_{ts=1}^{N_{ts}}\sum_{tr=0}^{Tz-1}\sum_{w=1}^{N_w} n(w,ts+tr,d)  p(z,ts|w,ts+tr,d)- \frac{\lambda_{z,d}}{ T_{ds} }
$$


$$
Take \space  \sum_{d=1}^{N_d}\mu_d \space back \space in \space to \space the \space formula \\
\space \\
\Rightarrow \sum_{tr=0}^{Tz-1}\sum_{w=1}^{N_w} n(w,ts+tr,d)  p(z,ts|w,ts+tr,d)- \frac{\lambda_{z,d}}{ T_{ds} }  = p(z,ts|d)\sum_{d=1}^{N_d}\mu_d\\
\space\\
\Rightarrow  p(z,ts|d) = \frac{\sum_{tr=0}^{Tz-1}\sum_{w=1}^{N_w} n(w,ts+tr,d)  p(z,ts|w,ts+tr,d)- \frac{\lambda_{z,d}}{ T_{ds} }}{\sum_{d=1}^{N_d}\mu_d}\\
\space\\
\Rightarrow  p(z,ts|d) = \frac{\sum_{tr=0}^{Tz-1}\sum_{w=1}^{N_w} n(w,ts+tr,d)  p(z,ts|w,ts+tr,d)- \frac{\lambda_{z,d}}{ T_{ds} }}{\sum_{z=1}^{N_z} \sum_{ts=1}^{N_{ts}}\sum_{tr=0}^{Tz-1}\sum_{w=1}^{N_w} n(w,ts+tr,d)  p(z,ts|w,ts+tr,d)- \frac{\lambda_{z,d}}{ T_{ds} }}
$$


The result can be expressed as:


$$
p(z,ts|d) \propto \sum_{tr=0}^{Tz-1}\sum_{w=1}^{N_w} n(w,ts+tr,d)  p(z,ts|w,ts+tr,d)- \frac{\lambda_{z,d}}{T_{ds}}
$$




#### Build Lagrange function 2



Note:

when the table z_ts is considered as two kinds of parameters: p(z|d) and p(ts|z,d), the primal table z_ts should be considered as two tables instead of one table z_ts. 

This is why we have two new constraints instead of one.


$$
\sum_{z=1}^{N_z} p(z|d)) = 1 \\
\space\\
\sum_{ts=1}^{N_{ts}} p(ts|z,d)) = 1
$$




The primal Lagrange function is:



$$
Lag = E[\mathcal{L}] + \sum_{z=1}^{N_z}\lambda_z(1 -\sum_{w=1}^{N_w} \sum_{tr=1}^{N_{tr}} p(w,tr|z) ) +\\ \sum_{d=1}^{N_d}\mu_d(1-\sum_{z=1}^{N_z} \sum_{ts=1}^{N_{ts}} p(z,ts|d)) \tag{10}
$$



Split p(z,ts|d) into two：




$$
= \sum_{d=1}^{N_d}\sum_{ta=1}^{N_{Ta}}\sum_{w=1}^{N_w} n(w,ta,d) \sum_{z=1}^{N_z} \sum_{ts=1}^{N_{ts}} p(z,ts|w,ta,d) log\space[p(d)p(z|d)p(ts|z,d)p(w,tr|z)]\\-\sum_{t_s, z,d} \frac{\lambda_{z, d}}{T_{d s}} \cdot \log \left(p\left(t_s|z,d\right)\right) 
+ \sum_{z=1}^{N_z}\lambda_z (1 -\sum_{w=1}^{N_w} \sum_{tr=1}^{N_{tr}} p(w,tr|z) )  +\\ \sum_{d=1}^{N_d}\mu_d(1-\sum_{z=1}^{N_z} p(z|d)) + \sum_{d=1}^{N_d}\alpha_d(1- \sum_{ts=1}^{N_{ts}} p(ts|z,d)) \tag{11}
$$




#### Solve P(z|d)





$$
\frac{\partial {\cal{Lag}}}{\partial p(z|d)} \Rightarrow \frac{\sum_{ts=1}^{N_{ts}}\sum_{tr=0}^{Tz-1}\sum_{w=1}^{N_w}n(w,ts+tr,d)  p(z,ts|w,ts+tr,d) }{p(z|d)} \\
\space \\- \sum_{d=1}^{N_d}\mu_d = 0 \\
\space\\
\Rightarrow \sum_{ts=1}^{N_{ts}}\sum_{tr=0}^{Tz-1}\sum_{w=1}^{N_w}n(w,ts+tr,d)  p(z,ts|w,ts+tr,d)  = p(z|d) \sum_{d=1}^{N_d}\mu_d
$$

$$
\\
\space\\
Introduce \space the \space contraint \space \sum_{z=1}^{N_z} p(z|d)\\
\space\\
\Rightarrow \sum_{z=1}^{N_z}\sum_{ts=1}^{N_{ts}}\sum_{tr=0}^{Tz-1}\sum_{w=1}^{N_w}n(w,ts+tr,d)  p(z,ts|w,ts+tr,d)  = [\sum_{d=1}^{N_d}\mu_d]\underbrace{\sum_{z=1}^{N_z}p(z|d) }_1 \\
\space\\
\Rightarrow \sum_{d=1}^{N_d}\mu_d = \sum_{z=1}^{N_z}\sum_{ts=1}^{N_{ts}}\sum_{tr=0}^{Tz-1}\sum_{w=1}^{N_w}n(w,ts+tr,d)  p(z,ts|w,ts+tr,d)
$$

$$
Take \space  \sum_{d=1}^{N_d}\mu_d \space back \space in \space to \space the \space formula \\
\space \\
\Rightarrow \sum_{ts=1}^{N_{ts}}\sum_{tr=0}^{Tz-1}\sum_{w=1}^{N_w}n(w,ts+tr,d)  p(z,ts|w,ts+tr,d)  = p(z|d) \sum_{d=1}^{N_d}\mu_d\\
\space\\
\Rightarrow p(z|d) = \frac{\sum_{ts=1}^{N_{ts}}\sum_{tr=0}^{Tz-1}\sum_{w=1}^{N_w}n(w,ts+tr,d)  p(z,ts|w,ts+tr,d)}{\sum_{d=1}^{N_d}\mu_d}\\
\space\\
p(z|d) = \frac{\sum_{ts=1}^{N_{ts}}\sum_{tr=0}^{Tz-1}\sum_{w=1}^{N_w}n(w,ts+tr,d)  p(z,ts|w,ts+tr,d)}{\sum_{z=1}^{N_z}\sum_{ts=1}^{N_{ts}}\sum_{tr=0}^{Tz-1}\sum_{w=1}^{N_w}n(w,ts+tr,d)  p(z,ts|w,ts+tr,d)}
$$



The result can be expressed as:



$$
p(z|d) \propto \sum_{ts=1}^{N_{ts}}\sum_{tr=0}^{Tz-1}\sum_{w=1}^{N_w}n(w,ts+tr,d)  p(z,ts|w,ts+tr,d)
$$




#### Solve P(ts|z,d)




$$
=  \sum_{d=1}^{N_d}\sum_{tr=0}^{Tz-1}\sum_{w=1}^{N_w}\sum_{z=1}^{N_z} \sum_{ts=1}^{N_{ts}} n(w,ta,d) p(z,ts|w,ta,d) log\space[p(d)p(z|d)p(ts|z,d)p(w,tr|z)]\\-\sum_{t_s, z,d} \frac{\lambda_{z, d}}{T_{d s}} \cdot \log \left(p\left(t_s|z,d\right)\right) 
+ \sum_{d=1}^{N_d}\lambda_d (1 -\sum_{w=1}^{N_w} \sum_{tr=1}^{N_{tr}} p(w,tr|z) )  +\\ \sum_{d=1}^{N_d}\mu_d(1-\sum_{z=1}^{N_z} p(z|d)) + \sum_{d=1}^{N_d}\alpha_d(1- \sum_{ts=1}^{N_{ts}} p(ts|z,d)) \tag{11}
$$





$$
\frac{\partial {\cal{Lag}}}{\partial p(ts|z,d)} \Rightarrow \frac{\sum_{tr=0}^{Tz-1}\sum_{w=1}^{N_w}n(w,ts+tr,d)  p(z,ts|w,ts+tr,d) }{p(ts|z,d)} \\
\space \\- \frac{\lambda_{z,d}}{T_{ds}\space p(ts|z,d)} - \sum_{d=1}^{N_d}\alpha_d = 0 \\
\space\\
\Rightarrow \sum_{tr=0}^{Tz-1}\sum_{w=1}^{N_w}n(w,ts+tr,d)  p(z,ts|w,ts+tr,d)- \frac{\lambda_{z,d}}{T_{ds}}  = p(ts|z,d) \sum_{d=1}^{N_d}\alpha_d\\
\space\\
Introduce \space the \space contraint \space \sum_{ts=1}^{N_{ts}} p(ts|z,d)\\
\space\\
\Rightarrow \sum_{ts=1}^{N_{ts}}  \sum_{tr=0}^{Tz-1}\sum_{w=1}^{N_w}n(w,ts+tr,d)  p(z,ts|w,ts+tr,d)- \frac{\lambda_{z,d}}{T_{ds}} =\underbrace{\sum_{ts=1}^{N_{ts}} p(ts|z,d)}_1\sum_{d=1}^{N_d}\alpha_d\\
\space\\
\Rightarrow \sum_{d=1}^{N_d}\alpha_d = \sum_{ts=1}^{N_{ts}}  \sum_{tr=0}^{Tz-1}\sum_{w=1}^{N_w}n(w,ts+tr,d) n(w,ts+tr,d)  p(z,ts|w,ts+tr,d)- \frac{\lambda_{z,d}}{T_{ds}}
$$




$$
Take \space  \sum_{d=1}^{N_d}\alpha_d \space back \space in \space to \space the \space formula \\
\space \\
\Rightarrow \sum_{tr=0}^{Tz-1}\sum_{w=1}^{N_w}n(w,ts+tr,d)  p(z,ts|w,ts+tr,d)- \frac{\lambda_{z,d}}{T_{ds}}  = p(ts|z,d) \sum_{d=1}^{N_d}\alpha_d\\
\space\\
p(ts|z,d) = \frac{\sum_{tr=0}^{Tz-1}\sum_{w=1}^{N_w} n(w,ts+tr,d)  p(z,ts|w,ts+tr,d)- \frac{\lambda_{z,d}}{T_{ds}}}{\sum_{d=1}^{N_d}\alpha_d}\\
\space\\
p(ts|z,d) = \frac{\sum_{tr=0}^{Tz-1}\sum_{w=1}^{N_w} n(w,ts+tr,d)  p(z,ts|w,ts+tr,d)- \frac{\lambda_{z,d}}{T_{ds}}}{ \sum_{ts=1}^{N_{ts}}  \sum_{tr=0}^{Tz-1}\sum_{w=1}^{N_w}n(w,ts+tr,d) p(z,ts|w,ts+tr,d)- \frac{\lambda_{z,d}}{T_{ds}}}
$$



The result can be expressed as:



$$
p(ts|z,d) \propto \sum_{tr=0}^{Tz-1}\sum_{w=1}^{N_w}n(w,ts+tr,d) n(w,ts+tr,d)  p(z,ts|w,ts+tr,d)- \frac{\lambda_{z,d}}{T_{ds}}
$$





# Reference 

[A Sequential Topic Model for Mining Recurrent Activities from Video and Audio Data Logs.
Jagannadan Varadarajan,Remi Emonet,Jean-Marc Odobez](<https://pdfs.semanticscholar.org/994c/8fbd9cb8d51356ed22ca93bbfbfb82e36f68.pdf>)