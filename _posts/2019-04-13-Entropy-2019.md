---
layout:     post   				    # 使用的布局（不需要改）
title:      Entropy 				# 标题 
subtitle:   Info Entropy, KL Divergence #副标题
date:       2019-04-14 				# 时间
author:     WYX 						# 作者
header-img: img/1.1.jpeg	                 #这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - PGM, Entropy
---

# Entropy 

熵表示从"不知道"到"知道"过程中所付出的代价。度量了**系统不确定性** ,同样也可以解释为是描述了系统的**无序程度**。熵越大，则表示系统的不确定性越大。



## 信息熵


$$
\sum_{k=1}^K p_k log_2 \frac{1}{p_k} = -\sum_{k=1}^N log_2 p_k
$$



上述 $p_k$ 为事件的分布，在主题模型当中，事件的分布一般为离散分布。

## 交叉熵



交叉熵表示，在给定的**真实分布**下，使用**非真实分布**消除系统的不确定性所需要付出的努力的大小


$$
\sum_{k=1}^K p_k \frac{1}{log_2q_k} = - \sum_{k=1}^K p_k {log_2q_k}
$$


其中，上式 $q_k$ 为**非真实分布**，$p_k$ 为真实分布。

## KL Divergence



**相对熵用来衡量两个概率分布之间的差异**，但是KL 散度（相对熵）不具有对称性，所以不能作为距离度量。

$$
\mathbb{KL}(p||q) = H(p,q) - H(p)
$$

$$
=\sum_{k=1}^K p_k log_2\frac{1}{q_k} - \sum_{k=1}^K p_k log_2\frac{1}{p_k}
$$

$$
= \sum_{k=1}^K p_k log_2{p_k} - \sum_{k=1}^K p_k log_2 q_k
$$

$$
= \sum_{k=1}^K p_k log_2\frac{p_k}{q_k}
$$



其中，$p$ 为真实分布，$q$ 为 wanted distribution. 如果 $q$ 越接近真实分布，其 $\mathbb{KL}$ 散度就越低，理想情况为 0

# Reference

1. [wikipedia](<https://zh.wikipedia.org/wiki/%E7%86%B5_(%E4%BF%A1%E6%81%AF%E8%AE%BA)>)

2. [【直观详解】信息熵、交叉熵和相对熵](<https://charlesliuyx.github.io/2017/09/11/%E4%BB%80%E4%B9%88%E6%98%AF%E4%BF%A1%E6%81%AF%E7%86%B5%E3%80%81%E4%BA%A4%E5%8F%89%E7%86%B5%E5%92%8C%E7%9B%B8%E5%AF%B9%E7%86%B5/>)
