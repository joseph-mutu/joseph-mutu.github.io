---
layout:     post   				    # 使用的布局（不需要改）
title:      Entropy 				# 标题 
subtitle:   Info Entropy, KL Divergence #副标题
date:       2019-04-14 				# 时间
author:     WYX 						# 作者
header-img: img/post-bg-2015.jpg 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - PGM, Entropy
---

# Entropy 

熵表示从"不知道"到"知道"过程中所付出的代价。度量了**系统不确定性**[^1] ,同样也可以解释为是描述了系统的**无序程度**。熵越大，则表示系统的不确定性越大。

# 

## 信息熵


$$
\sum_{k=1}^K p_k log_2 \frac{1}{p_k} = -\sum_{k=1}^N log_2 p_k
$$



上述 $p_k$ 为事件的分布，在主题模型当中，事件的分布一般为离散分布。下图计算了不同分布之间的信息熵[^2]

![1555256274391](C:\Users\mutudeh\AppData\Roaming\Typora\typora-user-images\1555256274391.png)

## 交叉熵



交叉熵表示，在给定的**真实分布**下，使用**非真实分布**消除系统的不确定性所需要付出的努力的大小


$$
\sum_{k=1}^K q_k \frac{1}{log_2p_k} = - \sum_{k=1}^K q_k {log_2p_k}
$$


其中，上式 $q_k$ 为** 非真实分布**，$p_k$ 为真实分布。下图计算了不同分布之间的交叉熵

![1555256359892](C:\Users\mutudeh\AppData\Roaming\Typora\typora-user-images\1555256359892.png)





# Reference

1. [wikipedia](<https://zh.wikipedia.org/wiki/%E7%86%B5_(%E4%BF%A1%E6%81%AF%E8%AE%BA)>)

2. [【直观详解】信息熵、交叉熵和相对熵](<https://charlesliuyx.github.io/2017/09/11/%E4%BB%80%E4%B9%88%E6%98%AF%E4%BF%A1%E6%81%AF%E7%86%B5%E3%80%81%E4%BA%A4%E5%8F%89%E7%86%B5%E5%92%8C%E7%9B%B8%E5%AF%B9%E7%86%B5/>)

