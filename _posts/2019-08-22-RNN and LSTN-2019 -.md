---
layout:     post   				    # 使用的布局（不需要改）
title:      RNN 和 LSTM 总结				# 标题 
subtitle:   RNN, LSTM, gradient vanishing         #副标题
date:       2019-08-22 				# 时间
author:     WYX 						# 作者
header-img: img/5.10.png 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - CS224N,LSTM,RNN,gradient vanishing, BPTT


---



# RNN and LSTM

## RNN

RNN 网络的特点在于

- 增加了隐藏层的记忆单元
- 对输入进行序列化的处理。也就是考虑了时间
  - 比如一句话，"arrive Taipei on November 2th" 和 "Leave Taipei on November 2th" 如果不考虑 Taipei 之前的单词，则整句话的意思会有偏差



以如下简单的神经网络举例。下图中**蓝色的方块**代表记忆单元，初始值为 0 。加入记忆单元之后，输入向量顺序的不同也会造成输出向量的不同。

<center>
    <img src = "https://github.com/joseph-mutu/Pics/raw/master/RNN1.png" width = "400"/><img src = "https://github.com/joseph-mutu/Pics/raw/master/RNN3.png" width = "400"/>
</center>



对于输入的句子，RNN 按照 左 $\rightarrow$ 右 或者 左 $\leftarrow$ 右 的顺序依次处理每一个单词向量。在下图左中，记忆单元存储的是前一个单词在隐藏层所得到的结果，然后将该结果存储，在处理下一个单词时，作为输入的一部分。

现定义部分数学符号：

- $W_x$ 表示从输入向量 $x^i$ 向隐藏层 $a^i$ 的转移（**下图中黄色**）
- $W_h$ 表示将前一层隐藏层的值 $a^{i-1}$ 向当前隐藏层 $a^i$ 转移的参数矩阵（**下图中蓝色**）
- $W_s$ 表示从当前隐藏层到当前输出层的参数矩阵（**下图中绿色**）
- $a^i$ 表示隐藏层
- $y^i$ 表示时间 i 点的输出

则有：


$$
a^i  = \delta(W_x x^i + W_h a^{i-1}) \\
\space \\
y^i = softmax(W_s a^i)
$$
下图中的 $W_e$ 即为 $W_x$。 **U** 为$W_s$



<center>
    <img src = "https://github.com/joseph-mutu/Pics/raw/master/RNN7.png" width = "300"/>
</center>



### RNN 梯度消失$^{[2]}$

以一个单隐藏层的 RNN 进行说明。其损失函数计算如下


$$
L_i = \frac{1}{2} (\hat{y}_t - O_t)^2
\space \\
\mathcal{L} = \sum \limits_{i= 1}^3 L_i\\
$$
其中 $\mathcal{L}$ 为在 3 个时间点的损失的加和。



<center>
    <img src = "https://github.com/joseph-mutu/Pics/raw/master/RNN8.png" width = "600"/>
</center>

则上述 RNN 的前向过程可描述为：


$$
S_{1} = W_sS_0 + W_x x_1 \\
\space \\
O_1 = W_o S_1 \\
\space \\
S_{2} = W_sS_1 + W_x x_2 \\
\space \\
O_2 = W_o S_2 \\
\space \\
S_3 = W_sS_2 + W_x x_3 \\
\space \\
O_3 = W_o S_3 \\
$$






##### BackPropagation Through Time

以 $L_3$ 时刻的损失举例。

###### $W_o$


$$
\frac{\partial L_3}{\partial W_o} = \frac{\partial L_3}{\partial O_3}\frac{\partial O_3}{\partial W_3}
$$

###### $W_S$


$$
\space \\
\frac{\partial L_3}{W_S} = \frac{\partial L_3}{\partial O_3}\frac{\partial O_3}{\partial S_3}\frac{\partial S_3}{\partial W_S} 
+\frac{\partial L_3}{\partial O_3}\frac{\partial O_3}{\partial S_3}\frac{\partial S_3}{\partial S_2}\frac{\partial S_2}{\partial W_S} 
+\frac{\partial L_3}{\partial O_3}\frac{\partial O_3}{\partial S_3}\frac{\partial S_3}{\partial S_2}\frac{\partial S_2}{\partial S_1}\frac{\partial S_1}{\partial W_S}
$$


###### $W_x$ 


$$
\frac{\partial L_3}{\partial W_x} = \frac{\partial L_3}{\partial O_3}\frac{\partial O_3}{\partial S_3}\frac{\partial S_3}{\partial W_x} 
+\frac{\partial L_3}{\partial O_3}\frac{\partial O_3}{\partial S_3}\frac{\partial S_3}{\partial S_2}\frac{\partial S_2}{\partial W_x} 
+\frac{\partial L_3}{\partial O_3}\frac{\partial O_3}{\partial S_3}\frac{\partial S_3}{\partial S_2}\frac{\partial S_2}{\partial S_1}\frac{\partial S_1}{\partial W_x}
$$




### 多层 RNN$^{[3]}$

对于多层 RNN 网络，原理一样。拿下图右举例。设在 $x^t$ 时间点，竖直方向的隐藏层分别为 $a^{11},a^{12},a^{13},\ldots$ 不同层的隐藏层的值均存储进一个记忆单元。当计算 $x^{t+1}$ 时的**同层神经元**时，将**相应的同层的记忆单元**的值传入



<center>
        <img src = "https://github.com/joseph-mutu/Pics/raw/master/RNN4.png" width = "400"/><img src = "https://github.com/joseph-mutu/Pics/raw/master/RNN5.png" width = "400"/>
</center>



**在 RNN 中每一个句子可以被视为一个 minibatch**。每输入一个句子中的单词 $x^t$，RNN 会输出一个相应的 $y^t$ 。而最终的 **Loss Function** 则是将所有的 $y^t$ 加和的交叉熵。数学表达如下：


$$
\mathcal{L} = -\frac{1}{T} \sum \limits_{t = 1}^T \sum \limits_{j = 1}^V (y_{t,j} \times \hat{y}_{t,j} )
$$


其中 T 为当前句子的长度。$y_{t,j}, \hat{y}_{t,j}$  均为向量中的一个分量。$y_t$ 为当前预测值，$\hat{y}_t$ 为真实值。这里的向量长度为 $V$ 。**V 的长度根据任务的不同也会有所不同**。比如，如果我们想用 RNN 预测下一个句子中的空格最有可能出现什么词，则 V 就是当前语料库的长度。$ y_t$ 表示不同单词出现在这个空格的概率。$\hat{y}_t $ 则为一个 one-hot 向量

 

### 双向 RNN



双向 RNN 与单向一样。相同的句子一个从左向右输入，一个从右向左输入。当双向 RNN 均完成前向传输之后，将**对应单词的输入**加和，然后计算最终的 **loss function**。



<center>
    <img src = "https://github.com/joseph-mutu/Pics/raw/master/RNN6.png" width = "500"/>
</center>



## Long Short-term Memory(LSTM)

LSTM 在普通 RNN 的基础上添加了如下元素：

- “**Input Gate**” ： 从输入向量  $x^t$ 到隐藏层的转移，**要输入多少**
- “**Forget Gate**”: 对于之前的前一层的记忆单元，我们是否要继续保留，还是根据当前值进行刷新
- “**Output Gate**” ：对于当前的隐藏层的结果，是否要进行输出。

根据下图，上述过程可以表示为：

-  **Input Gate**: $f(z_i) \times g(z)$ , 其中 **z** 为原始输入向量，$f(z_i)$ 为决定输入多少。
- **Forget Gate** ： $c' = g(z)f(z_i) + cf(z_f)$ ，其中 c 为之前的 memory 的存储值。
- **Output Gate**：$a = h(c')f(z_o)$



<center>
    <img src = "https://github.com/joseph-mutu/Pics/raw/master/LSTM2.png" width = "500"/>
</center>



#### LSTM 与普通神经网络

上述结构，可以视为 LSTM 中的一个神经元。则与普通神经网络相比，LSTM 网络的一个神经元有 4 个输入：输入向量，input gate，forget gate 和 output gate



<center>
    <img src = "https://github.com/joseph-mutu/Pics/raw/master/LSTM3.png" width = "400"/><img src = "https://github.com/joseph-mutu/Pics/raw/master/LSTM4.png" width = "400"/>
</center>





## Reference

1. [李宏毅机器学习Youtube](http://speech.ee.ntu.edu.tw/~tlkagk/talk.html)
2. [RNN梯度消失和爆炸的原因](https://zhuanlan.zhihu.com/p/28687529)

3. [零基础入门深度学习(5) - 循环神经网络](https://zybuluo.com/hanbingtao/note/541458)