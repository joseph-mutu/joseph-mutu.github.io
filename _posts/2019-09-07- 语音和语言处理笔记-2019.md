---
layout:     post   				    # 使用的布局（不需要改）
title:      语音和语言处理笔记			# 标题 
subtitle:   Speech and language Processing  #副标题
date:       2019-09-08 				# 时间
author:     WYX 						# 作者
header-img: img/5.6.jpg	                 #这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - NLP notes 
---



# Minimum Edit Distance 最短编辑距离

最短编辑距离用以衡量两个字符串之间的距离，从字符串 A 变换到字符串 B 所需要最少的操作步数。操作分为以下三种情况

- 删除一个字符，操作步数为 1
- 插入一个字符，操作步数为 1
- 将一个字符替换成另一个字符，操作步数为 2

最短编辑距离的计算采用动态规划的方法进行，**edit(i,j**) 表示从字符串 **i** 变到字符串 **j** 所需要的最小操作步数

## 现对 edit(i,j) 进行分析

#### edit(0,j) 

表示从一个空字符串变换到字符串 j 所需要的步骤：edit(0,j) = j，最小操作步骤为每次插入一个字符

#### edit(i,0)

表示从一个字符串 **i** 变换到空字符串所需要的步骤：edit(i,0) = i，最小操作步骤为每次删除一个字符

#### **i** 和 **j** 都只有一个字符

##### 字符不相等

edit(1,1) :$a \rightarrow b$   有几下几种可能

- <u>**edit(0,1)+1**</u>: **+1 表示首先将 a 删除**，然后得到一个空字符串,edit(0,1) 表示从这个空字符变到 b 的最短距离
- **<u>edit(1,0) +1</u>** : edit(1,0) 表示从 a 变换到空字符串这一步开始，**+1 表示在空字符的基础上插入 b**
- <u>**edit(0,0)+ 2**</u>: edit(0,0) 表示从两者皆为空字符的情况下开始，**+2 表示将 a 替换成 b**

##### 字符相等

edit(1,1) : $a\rightarrow a$ = edit(0,0) 因为当前字符串相等，所以无需进行任何操作

#### i 和  j 为多个字符的字符串 

**Note**: edit(i,j) 表示从i 字符串**变到** j 字符串的最短编辑距离，**从 i 已经变成了 j**

以 **i = acd**， **j = def** 举例，注意我们的目标是将 acd **变为** def

$acd \rightarrow def$ 存在以下几种可能：

- 删除最后一个字符 d，将 acd 变为 ac，然后取 ac 到 def 的距离 edit(3,3) = edit(2,3) + 1
- 先从 acd 变到 de，然后在 de 字符串后插入一个 f，edit(3,3) = edit(3,2) +1
- 取 ac 到 de 之间的距离，然后利用替换操作，将 d 替换成 f 



## 朴素贝叶斯

书中第 66 页开始

#### 朴素贝叶斯所采取的假设

- 假设文本中的单词无所谓位置，也就是 **词袋模型(Bag of Words)**
- 假设文本中的单词互相独立，即右式的成立 $P(w_1,w_2,w_3,\ldots,w_n|c) \prod \limits_{w_i \in Voc} P(w_i|c) $

---

在文本分类中，朴素贝叶斯的目的为：


$$
\hat{c} = \arg\max \limits_{c\in Class} P(c|d) \\
= \arg\max \limits_{c\in Class} \frac{P(c,d)}{P(d)} \\
\propto P(c,d) \\
\propto \underbrace{P(d|c)}_{Likelihood} \underbrace{P(c)}_{Prior}
$$


**Note**: 上式之所以将 $P(d)$ 去掉，改换成 $\propto$ ，是因为对于所有的类别来说，$P(d)$ 的值是不会改变的，将所有的 class 除以一个相同的数，并不会影响 $\arg\max$ 的结果

---

为了便于表示，将 $P(d|c)$ 表示为每一个单词在 c 中的概率，如下


$$
P(d|c) = P(w_1,w_2,w_3,\ldots,w_n|c)
$$
为了便于计算，进一步将**每一个单词假设为独立**，则上式可以写为


$$
P(w_1,w_2,w_3,\ldots,w_n|c) = P(w_1|c) * P(w_2|c) \ldots P(w_n|c) = \prod \limits_{w_i \in Voc} P(w_i|c)
$$


注意上式进行表示的是整个语料中的单词，并非是单独一个 class 中的所有单词

则其中单独每一个概率的计算可以利用频率进行估计


$$
P(w_i|c) = \frac{Count(w_i,c)}{\sum \limits_{w_i \in Voc}Count(w_i,c)}
$$
为了避免在测试数据中，概率 0 的出现，对上式进行 Laplace 平滑处理


$$
P(w_i|c) = \frac{Count(w_i,c) + 1}{\sum \limits_{w_i \in Voc}Count(w_i,c) + V}
$$
**Note**：所谓 Laplace 平滑，就是将每一个可能出现的单词的频率都加 1。因为假设整个语料中的单词都在当前的 c 中出现了一次，所以分母加上 V

----

#### 未知单词以及 The，a 之类的 Stop Words

- 在测试数据中出现未知单词，朴素贝叶斯做 **忽略** 处理
- 对 The，a 之类的高频单词以相同单词对待
  - 书中提到，使用 Stop list 提出这样的高频词并不会提升模型的性能，所以以相同单词对待

---



#### 情感分析中的朴素贝叶斯

与通常的朴素贝叶斯不同在于

- 去除了**每一个 document** 中的重复单词
  - **Note**：并非去除了所有重复的单词

<center>
    <img src = "https://github.com/joseph-mutu/Pics/raw/master/Speech%20and%20Language%20Processing/NB%20for%20Sentiment%20Analysis.jpg" width = "500"/>
</center>

##### Sentiment Lexicons（情感词典）P71

适用于训练数据不多的情况，Lexicons 包含了多数单词，已经被标记好 `Positive` 或者是 `Negative` 

---



## Precision, Recall and F-Score

### Precision 查准率 and Recall 查全率

<center>
    <img src = "https://github.com/joseph-mutu/Pics/raw/master/Speech%20and%20Language%20Processing/precision%20%20and%20recall1.jpg" width = "200"/>
</center>



如上图所示

- Precision 代表的是查准率，意即在所有 `预测正类` 中，预测正确的百分比
- Recall 代表的是查全率，意即在所有 `真正正类` 中，预测正确的百分比

如下图所示

- Accuracy  正确率代表在所有`正负类`中，预测正确的百分比
  - 由于数据的不均衡分布，accuracy 并不可靠，所以一般不采用
  - 考虑一个大小为 1000 的数据集中，只有 10 个正类，990 个负类，则分类器只要每次判别为负，Accuracy 可以高达 99%

<center>
    <img src = "https://github.com/joseph-mutu/Pics/raw/master/Speech%20and%20Language%20Processing/precision%20and%20recall%202.jpg" width = "600"/>
</center>

### F-score

F-score 来源于 `加权调和平均数`

#### 调和平均数

与平均数不同，调和平均数重视 `分母的不同` ，且调和平均数更倾向于较小的数字

举例如下：

顺流速度30，逆流速度20，平均速度为多少？

速度=距离/时间，因为同一距离，所以分子相同。

假设距离为1，那么顺流速度的分母为：1/30，逆流为：1/20，分母平均数为：（1/30+1/20）/2

再用分子1除以分母平均数得出平均速度：
$$
\frac{2}{\frac{1}{30}*\frac{1}{20}}
$$

#####  一般形式

调和平均数的一般形式可以写为
$$
HarmonicMean(a_1,a_2,\ldots,a_n) = \frac{n}{\frac{1}{a_1}*\frac{1}{a2} \ldots \frac{1}{a_n}}
$$

#### F-score

则 F-score 可以写为，其中 $\alpha$ 为权重
$$
F = \frac{1}{\alpha \frac{1}{P} + (1-\alpha)\frac{1}{R}}
$$
或者设 
$$
\beta^2 = \frac{1-\alpha}{\alpha} \\
F = \frac{(\beta^2+1) PR}{\beta^2P +R}
$$


## 多类别中的 evaluation P76

多类别中的 `Precision` 和 `Recall ` 分为 `macroaveraging` 以及 `microaveraging` 两种

下图显示了三种类别中的 `macroaveraging` 计算

<center>
    <img src = "https://github.com/joseph-mutu/Pics/raw/master/Speech%20and%20Language%20Processing/multi-class%20eval1.jpg" width = "500"/>
</center>

在 `macroaveraging` 中分别计算出所有的 precision 以及 Recall，然后直接除以 3 

而 `microaveraging` 则要将所有的 system yes 部分累积，这样容易造成对数据量大的类别的偏颇

<center>
    <img src = "https://github.com/joseph-mutu/Pics/raw/master/Speech%20and%20Language%20Processing/multi-class%20eval2.jpg"
</center>



2/（1/30+1/20)