---
layout:     post   				    # 使用的布局（不需要改）
title:      Word2vec 				# 标题 
subtitle:   CBOW, Skip-Gram          #副标题
date:       2019-07-05 				# 时间
author:     WYX 						# 作者
header-img: img/music1.png 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - neural network,nlp,word2vec

---



# Word2vec

本文旨在记录对 word2vec 算法的理解。具体涉及到以下两篇文章

- word2vec Parameter Learning Explained, Xin Rong
- CS224n, Word Vectors I: Introduction, SVD and Word2Vec

---

## Notation

word2vec 算法旨在利用向量形式表示单词或者中文字符。这种表示方法便于进一步进行数学运算。例如可以使用向量的点乘来计算两个单词之间的相似程度。点乘的值越大，则表示两向量越相似。


$$
a \cdot b = |a| |b| cos\theta
$$


为了加以区分 one-hot 向量表示与 Wrod2vec 向量，这里将 word2vec 的向量称为**真实向量**。



### word2vec 算法具有以下性质：

- 是一个简单的神经网络，其中神经网络的参数即为我们想要的**单词的向量**
- 神经网络的输入是单词的 one-hot 向量表示，输出则是真实向量
- Word2vec 有两种模型，分别为 CBOW 以及 Skip-Gram，区别如下
  - CBOW 输入多个单词的 one-hot，输出一个单词的真实向量，也就是给定多个单词，预测给定多个单词的语境下，某个单词出现的概率
  - Skip-Gram 输入为一个单词的 one-hot，输出则为 C 个真实向量（**但是这 C 个真实向量完全一致**）。C 的数目取决于你想用输入单词多少个单词
- 这里设定语料库的大小为 V
  - 所谓语料库，就是将文章中的词抽取出来，去除重复的词，排好顺序（任意顺序）。说明我们有 V 个不同的词，而且我们知道第 x 个词是什么
- 设定单词的真实向量长度为 N
  - 真实向量的长度可以自己定义



### Word2vec 的训练数据:

在监督式学习 (supervised model ) 的模型训练中，需要 (x,$ \hat{y} $) 的数据进行参数的学习。在 wrod2vec 中，x 为输入的 one-hot 向量。无论是 CBOW 还是 Skip-Gram 都是 one-hot 向量。同样地，y 也是不同的 one-hot 向量，代表不同的单词。

假设我们有以下的句子，一共有 5 个 单词 “Linghu Chong”, “is”,“the”,“best”,“swordsman”。 则 V 是 5


$$
Linghu ~ Chong ~ is ~ the ~ best ~ swordsman
$$




在叙述中会这样描述： 输入的单词为 "Linghu Chong" ，要预测出现 “swordsman” 的概率。 则


$$
输入 ~x ： [1,0,0,0,0] \\
标签~ \hat{y}：[0,0,0,0,1]
$$


## P($w_{out}$\|$w_{in}$)

CBOW 模型以及 Skip-Gram 模型分别对应着，多对一和一对多的情况。这里首先讨论给定一个单词，预测另个一单词的情况。然后进行扩展。

Note:

**in** 和 **out** 这里仅代表单词的编号。



![Sinble input sinple output](https://ae01.alicdn.com/kf/HTB1x6jRXmf2gK0jSZFP760sopXak.png)



上图[1]，中 X 即为输入单词的 one-hot，而输出 $\vec{y} = y_1,y_2,y_3,\ldots,y_V$ 则为真实向量（不是 label）。这里设 V =5， N = 4。则上图神经网络存在两个参数矩阵 $W,W'$。


$$
W = V \times N (input ~ vector) \\
W' = N \times V (output ~ vector)
$$


注意 $W$ 和 $W'$ 虽然具备相同的大小，但是是不同的参数矩阵。一般来说，一个语料库中的词 $w_i$ 对应**两组向量** 分别为 W 中的**第 i 行**以及 W' 中的**第 i 列**。取第一个参数矩阵的第 $i$ 行 即为我们所需要的 word vector。

![](https://ae01.alicdn.com/kf/HTB1vvTVXoT1gK0jSZFr763NCXXao.png)



#### Input $\rightarrow$ Hidden Layer

设输入的单词为 $w_1$ 的one-hot，预测单词为 $w_3$ 。则从 input 层到 hidden 层所做的运算，仅为提取 W 矩阵中标号为 $w_1$ 的单词向量。$W_{in} = X^T \times W$ 。图示如下

![input - hidden](https://ae01.alicdn.com/kf/HTB1.nvVXeP2gK0jSZFo761uIVXak.png)

#### Hidden $\rightarrow$ Output

从 Hidden 层到输出层，此时的输入即为 $W_{in}$ 对应的单词向量，维度大小为 $1 \times N$ 。

乘以 W' 矩阵，得到一个 $1 \times V$ 的向量 **u**，该向量的具体含义为：给定输入单词 $X_{in}$, 得到语料库中每一个单词的概率大小为多少。图示如下

![](https://ae01.alicdn.com/kf/HTB1UjPYXeP2gK0jSZFo761uIVXak.png)



#### 更新推导

首先明确一点，我们要更新就是两个矩阵，W 和 W‘ 中的每个值。怎么更新，根据 label $\hat{y}$ 。比如我们给定 （Linghu Chong, swordsman） 数据，表示我们在文章中观察到 “Linghu Chong” 后面跟着的是 “swordsman”。 （如何选定数据 , please check [here](https://zhuanlan.zhihu.com/p/27234078)）。

----

要更新参数矩阵，首先设定目标函数，此处，我们想要最大化一个概率 $p(W_{out} | W_{in})$ 。$W_{in} = Linghu ~ Chong$

$W_{out} = swordsman$ 。

根据上述图示，$p(W_{out} | W_{in})$ 可以写为：


$$
p(W_{out} | W_{in}) = softmax(W_{in} \times W'_{out}) \\
\space \\
\Rightarrow \frac{exp(W_{in} \times W'_{out})}{\sum \limits_V exp(W_{in} \times W')}
$$
则最大化 $p(W_{out} | W_{in})$  ：


$$
max ~ p(W_{out} | W_{in}) = max ~ log p(W_{out} | W_{in}) \\
\space \\
\Rightarrow max ~ \textbf{u}_{out} - log \sum \limits_V \textbf{u}
$$
设定 loss function 为：


$$
loss = log \sum \limits_V \textbf{u} - \textbf{u}_{out} \\
\space \\
where ~ \textbf{u}_{out} = W_{in} \times W'_{out}
$$


这里仅对上述 max 函数进行了 取反的操作，所以如今的目标从 max 变换为了 最小化 loss function

##### Update W'

**首先对 loss 函数求偏导**


$$
\frac{\partial loss}{\partial u_j} = y_{out} - \mathbb{\hat{Y}} = err_j
$$


其中当 $u_j = u_{out} 时， \mathbb{\hat{Y}}$ 为 1，也就是 label。这一偏导的实际意义就是，输出值与真实值的误差。

**对 W’ 求偏导**


$$
\frac{\partial loss}{\partial W'} = \frac{\partial loss}{\partial u_j} \frac{\partial u_j}{\partial W'} = err_j \times W_{in}
$$


其中 $u_j$ 为未做 softmax 之前的 $W_{in} \times W'$ 的一个分量。 



# Reference

1. [word2vec Parameter Learning Explained](https://arxiv.org/pdf/1411.2738.pdf)

2. [CS224n, Word Vectors I: Introduction, SVD and Word2Vec](https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf)
3. [理解 Word2Vec 之 Skip-Gram 模型](https://zhuanlan.zhihu.com/p/27234078)