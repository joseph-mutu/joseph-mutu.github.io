---
layout:     post   				    # 使用的布局（不需要改）
title:      Word2vec 				# 标题 
subtitle:   CBOW, Skip-Gram          #副标题
date:       2019-07-05 				# 时间
author:     WYX 						# 作者
header-img: img/music1.png 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - neural network,nlp,word2vec

---



# Word2vec

本文旨在记录对 word2vec 算法的理解。具体涉及到以下两篇文章

- word2vec Parameter Learning Explained, Xin Rong
- CS224n, Word Vectors I: Introduction, SVD and Word2Vec

---

## Notation

word2vec 算法旨在利用向量形式表示单词或者中文字符。这种表示方法便于进一步进行数学运算。例如可以使用向量的点乘来计算两个单词之间的相似程度。点乘的值越大，则表示两向量越相似。


$$
a \cdot b = |a| |b| cos\theta
$$


为了加以区分 one-hot 向量表示与 Wrod2vec 向量，这里将 word2vec 的向量称为**真实向量**。



### word2vec 算法具有以下性质：

- 是一个简单的神经网络，其中神经网络的参数即为我们想要的**单词的向量**
- 神经网络的输入是单词的 one-hot 向量表示，输出则是真实向量
- Word2vec 有两种模型，分别为 CBOW 以及 Skip-Gram，区别如下
  - CBOW 输入多个单词的 one-hot，输出一个单词的真实向量，也就是给定多个单词，预测给定多个单词的语境下，某个单词出现的概率
  - Skip-Gram 输入为一个单词的 one-hot，输出则为 C 个真实向量（**但是这 C 个真实向量完全一致**）。C 的数目取决于你想用输入单词多少个单词
- 这里设定语料库的大小为 V
  - 所谓语料库，就是将文章中的词抽取出来，去除重复的词，排好顺序（任意顺序）。说明我们有 V 个不同的词，而且我们知道第 x 个词是什么
- 设定单词的真实向量长度为 N
  - 真实向量的长度可以自己定义



### Word2vec 的训练数据:

在监督式学习 (supervised model ) 的模型训练中，需要 (x,y) 的数据进行参数的学习。在 wrod2vec 中，x 为输入的 one-hot 向量。无论是 CBOW 还是 Skip-Gram 都是 one-hot 向量。同样地，y 也是不同的 one-hot 向量，代表不同的单词。

假设我们有以下的句子，一共有 5 个 单词 “Linghu Chong”, “is”,“the”,“best”,“swordsman”。 则 V 是 5


$$
Linghu ~ Chong ~ is ~ the ~ best ~ swordsman
$$




在叙述中会这样描述： 输入的单词为 "Linghu Chong" ，要预测出现 “swordsman” 的概率。 则


$$
输入 ~x ： [1,0,0,0,0] \\
标签~ y：[0,0,0,0,1]
$$








# Reference

[word2vec Parameter Learning Explained](https://arxiv.org/pdf/1411.2738.pdf)

[CS224n, Word Vectors I: Introduction, SVD and Word2Vec](https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf)