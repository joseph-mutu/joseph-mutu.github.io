---
layout:     post   				    # 使用的布局（不需要改）
title:      Word2vec 				# 标题 
subtitle:   CBOW, Skip-Gram          #副标题
date:       2019-07-05 				# 时间
author:     WYX 						# 作者
header-img: img/music1.png 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - neural network,nlp,word2vec

---



# Word2vec

本文旨在记录对 word2vec 算法的理解。具体涉及到以下两篇文章

- word2vec Parameter Learning Explained, Xin Rong
- CS224n, Word Vectors I: Introduction, SVD and Word2Vec

---

## Notation

word2vec 算法旨在利用向量形式表示单词或者中文字符。这种表示方法便于进一步进行数学运算。例如可以使用向量的点乘来计算两个单词之间的相似程度。点乘的值越大，则表示两向量越相似。


$$
a \cdot b = |a| |b| cos\theta
$$


为了加以区分 one-hot 向量表示与 Wrod2vec 向量，这里将 word2vec 的向量称为**真实向量**。



### word2vec 算法具有以下性质：

- 是一个简单的神经网络，其中神经网络的参数即为我们想要的**单词的向量**
- 神经网络的输入是单词的 one-hot 向量表示，输出则是真实向量
- Word2vec 有两种模型，分别为 CBOW 以及 Skip-Gram，区别如下
  - CBOW 输入多个单词的 one-hot，输出一个单词的真实向量，也就是给定多个单词，预测给定多个单词的语境下，某个单词出现的概率
  - Skip-Gram 输入为一个单词的 one-hot，输出则为 C 个真实向量（**但是这 C 个真实向量完全一致**）。C 的数目取决于你想用输入单词多少个单词
- 这里设定语料库的大小为 V
  - 所谓语料库，就是将文章中的词抽取出来，去除重复的词，排好顺序（任意顺序）。说明我们有 V 个不同的词，而且我们知道第 x 个词是什么
- 设定单词的真实向量长度为 N
  - 真实向量的长度可以自己定义



### Word2vec 的训练数据:

在监督式学习 (supervised model ) 的模型训练中，需要 (x,$ \hat{y} $) 的数据进行参数的学习。在 wrod2vec 中，x 为输入的 one-hot 向量。无论是 CBOW 还是 Skip-Gram 都是 one-hot 向量。同样地，y 也是不同的 one-hot 向量，代表不同的单词。

假设我们有以下的句子，一共有 5 个 单词 “Linghu Chong”, “is”,“the”,“best”,“swordsman”。 则 V 是 5


$$
Linghu ~ Chong ~ is ~ the ~ best ~ swordsman
$$




在叙述中会这样描述： 输入的单词为 "Linghu Chong" ，要预测出现 “swordsman” 的概率。 则


$$
输入 ~x ： [1,0,0,0,0] \\
标签~ \hat{y}：[0,0,0,0,1]
$$


## P($w_{out}$\|$w_{in}$)

CBOW 模型以及 Skip-Gram 模型分别对应着，多对一和一对多的情况。这里首先讨论给定一个单词，预测另个一单词的情况。然后进行扩展。

Note:

**in** 和 **out** 这里仅代表单词的编号。



![Sinble input sinple output](https://ae01.alicdn.com/kf/HTB1x6jRXmf2gK0jSZFP760sopXak.png)



上图[1]，中 X 即为输入单词的 one-hot，而输出 $\vec{y} = y_1,y_2,y_3,\ldots,y_V$ 则为真实向量（不是 label）。这里设 V =5， N = 4。则上图神经网络存在两个参数矩阵 $W,W'$。


$$
W = V \times N (input ~ vector) \\
W' = N \times V (output ~ vector)
$$


注意 $W$ 和 $W'$ 虽然具备相同的大小，但是是不同的参数矩阵。一般来说，一个语料库中的词 $w_i$ 对应**两组向量** 分别为 W 中的**第 i 行**以及 W' 中的**第 i 列**。取第一个参数矩阵的第 $i$ 行 即为我们所需要的 word vector。

![](https://ae01.alicdn.com/kf/HTB1vvTVXoT1gK0jSZFr763NCXXao.png)



#### Input $\rightarrow$ Hidden Layer

设输入的单词为 $w_1$ 的one-hot，预测单词为 $w_3$ 。则从 input 层到 hidden 层所做的运算，仅为提取 W 矩阵中标号为 $w_1$ 的单词向量。$W_{in} = X^T \times W$ 。图示如下

![input - hidden](https://ae01.alicdn.com/kf/HTB1.nvVXeP2gK0jSZFo761uIVXak.png)

$$
h_1 = W_{11}*1 + W_{21}*0 + W_{31}*0 +  W_{41}*0 +  W_{51}*0 \\
\space \\
h_2 = W_{12}*1 + W_{22}*0 + W_{32}*0 +  W_{42}*0 +  W_{52}*0 \\
\space \\
h_3 = W_{13}*1 + W_{23}*0 + W_{33}*0 +  W_{43}*0 +  W_{53}*0 \\
\space \\
h_4 = W_{14}*1 + W_{24}*0 + W_{34}*0 +  W_{44}*0 +  W_{54}*0 \\
\space \\
h = [h_1,h_2,h_3,h_4]
$$






#### Hidden $\rightarrow$ Output

从 Hidden 层到输出层，此时的输入即为 $W_{in}$ 对应的单词向量，维度大小为 $1 \times N$ 。

乘以 W' 矩阵，得到一个 $1 \times V$ 的向量 **u**，该向量的具体含义为：给定输入单词 $X_{in}$, 得到语料库中每一个单词的概率大小为多少。图示如下

![](https://ae01.alicdn.com/kf/HTB1UjPYXeP2gK0jSZFo761uIVXak.png)



从 Hidden $\rightarrow$ Output 可以表示为


$$
\textbf{u} = h \times W'
$$


其中 $\textbf{u}$ 存在 V 个分量，具体可写为


$$
u_1 = h_1 * W'_{11} + h_2 * W'_{21} + h_3 * W'_{31} + h_4*W'_{41} \\
\space \\
u_2 = h_1 * W'_{12} + h_2 * W'_{22} + h_3 * W'_{32} + h_4*W'_{42} \\
\space \\
u_3 = h_1 * W'_{13} + h_2 * W'_{23} + h_3 * W'_{33} + h_4*W'_{43} \\
\space \\
u_4 = h_1 * W'_{14} + h_2 * W'_{24} + h_3 * W'_{34} + h_4*W'_{44} \\
\space \\
u_5 = h_1 * W'_{15} + h_2 * W'_{25} + h_3 * W'_{35} + h_4*W'_{45} \\
\space \\
$$


经过 softmax 之后，将 $\textbf{u}$ 转换为概率


$$
y_1 = p(w_1|w_{in}) = \frac{exp{(u_1)}}{\sum_{i=1}^V exp{(u_i)}}
$$


设定目标函数为最大化 $p(w_{out} $\|$ w_{in})$  ：




$$
\mathcal{L} = max ~ p(w_{out} | w_{in}) = max ~ log p(w_{out} | w_{in}) \\
\space \\
\mathcal{L} =  \max ~ \textbf{u}_{out} - log \sum \limits_V exp\textbf{u}_V
$$

为了方便起见，将目标函数取反，从而使得目标变化最小化 -$\mathcal{L}$ 


$$
\mathbb{L} = -\mathcal{L} = \min log\sum_V exp \textbf{u}_V - u_{out}
$$




**Note: out 和 in 在这里只代表输入单词和输出单词的索引**



#### 更新推导

首先明确一点，我们要更新就是两个矩阵，W 和 W‘ 中的每个值。怎么更新，根据 label $\hat{y}$ 。比如我们给定 （Linghu Chong, swordsman） 数据，表示我们在文章中观察到 “Linghu Chong” 后面跟着的是 “swordsman”。 （如何选定数据 , please check [here](https://zhuanlan.zhihu.com/p/27234078)）。

------

要更新参数矩阵，首先设定目标函数。给定一个输入单词 $w_{in}$ , 根据真实的文本，可以判断一个真实的输出单词 $W_{out}$。目标函数设定为最大化概率 $p(w_{out}$ \|$ w_{in})$ 。$w_{in} = Linghu ~ Chong$，$w_{out} = swordsman$ 。



##### 更新 W‘

对 $W'_{ij}$ 取梯度。


$$
\frac {\partial \mathbb{L}}{W'_{ij}} = \frac{\partial \mathbb{L}}{\partial u_{j}} \cdot \frac{\partial u_{j}}{\partial{W'_{ij}}}
$$


$W'_{ij}$ 中 j 为单词的个数，i 为单词向量的维度。需要对 W’ 矩阵（$N \times V$）中的每一个值进行更新



首先看第一项



$$
\frac{\partial \mathbb{L}}{\partial {u_j}} =  y_j - \mathbb{T_j} - = e_j
$$



其中 $e_j$ 代表误差

Note: 这里的 $\mathbb{T_j}$ 表示的是一个标示量。当且仅当，j = out 时（j 和 out 都代表单词的索引），$\mathbb{T_j} = 1$ ，其他时候 $T_j = 0$ .



看第二项


$$ { }
\frac{\partial u_j }{ \partial W'_{ij}} = h_i
$$


Note: $u_1$ 只能对 $W'_{i1}$ 进行更新，对其他如 $W'_{i2,i3,i4}$  取导数为 0

因此 W‘ 矩阵的更新为


$$
W'_{ij}{^{new}} = W'_{ij}{^{old}} - \eta \cdot e_j \cdot h_i
$$
其中 $\eta$ 为步长



**更新 W**

更新 W 的时候有一点需要注意，因为 $u_1 \sim u_5$ 均和 $h_{1\sim 5}$ 有关，所以需要应用链式法则的时候需要连加，如下


$$
\frac{\partial\mathbb{L}}{\partial h_i} = \sum_{j=1}^V \frac{\partial \mathbb{L}}{\partial u_j} \cdot \frac{\partial u_j}{h_i}
$$

----

这个地方有些模糊，这里举一个例子


$$
\mathcal{l} = h_1 + h2 \\
\space \\
h_1 = 4 \times b_1, h_2 = 3\times b_1 +  4 \times b_2
$$


若对 $\mathcal{l}$ 取关于 $b_1$ 的导数，则为


$$
\frac{\partial \mathcal{l}}{\partial b_1} = \frac{\partial \mathcal{l}}{\partial h_1} \cdot \frac{\partial h_1}{\partial b_1} + \frac{\partial \mathcal{l}}{\partial h_2}\frac{\partial h_2}{\partial b_1}
$$

---

取得关于关于每个 h 的导数之后，从 h 求关于 $W_{ij}$ 的导数，


$$
\frac{\partial h_i}{\partial W_{ki}} = x_k \times \frac{\partial \mathbb{L}}{\partial h_i}
$$








# Reference

1. [word2vec Parameter Learning Explained](https://arxiv.org/pdf/1411.2738.pdf)

2. [CS224n, Word Vectors I: Introduction, SVD and Word2Vec](https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf)
3. [理解 Word2Vec 之 Skip-Gram 模型](https://zhuanlan.zhihu.com/p/27234078)