---
layout:     post   				    # 使用的布局（不需要改）
title:      机器学习基础				# 标题 
subtitle:   machine learning           #副标题
date:       2020-03-02 				# 时间
author:     WYX 						# 作者
header-img: img/6.1.png 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - Machine Learning 

---

---

# 机器学习基础

本文旨在记录机器学习的各种基础知识，内容较杂，主要用于查阅

## Pytorch 

### Torch.nn

Torch.nn 的官方文档 [here](https://pytorch.org/docs/stable/nn.html#)

#### CrossEntropyLoss

一般计算交叉熵会采用下式，其中 p(x) 为真实分布，q(x) 为预测分布，x 表示 class 类别


$$
H(p,q) = -\sum_x p(x)logq(x) \tag{1}
$$


因为一般情况下，真实分布是 one-hot 向量，也就是只有一个值为真，所以 (1) 式也可以写为


$$
(1) = -logq(m),m 为真实分布值为 1 对应的~ index
$$


要使得分类器输出的预测值成为一个预测分布 q(x)，要对其输出采取 `softmax` 操作，如下
$$
q(x) = \frac{e^{q(x)}}{\sum_x e^{q(x)}}
$$
**对比 Torch.nn 官方文档的交叉熵损失函数**


$$
\operatorname{loss}(x, \text { class })=-\log \left(\frac{\exp (x[\text { ctass })]}{\sum_{j} \exp (x[j])}\right)=-x[\text { class }]+\log \left(\sum_{j} \exp (x[j])\right)
$$


其中 x 表示预测输出，class 表示 index

针对样本不均衡的问题，可以采用 weight 的形式，如下


$$
\operatorname{loss}(x, \text { class })=\text {weight}[\text {class}]\left(-x[\text {class}]+\log \left(\sum_{j} \exp (x[j])\right)\right)
$$


### Tensor 相关

Pytorch 的 tensor 官方文档 [Here](https://pytorch-cn.readthedocs.io/zh/latest/package_references/Tensor/)

**！注意：** 会改变tensor的函数操作会用一个下划线后缀来标示。比如，`torch.FloatTensor.abs_()`会在原地计算绝对值，并返回改变后的tensor，而`tensor.FloatTensor.abs()`将会在一个新的tensor中计算结果

#### Moving Average

对下列代码进行解读

```python
running_loss = 0.0

for batch in generate_batch:
	loss = loss_func(batch,y_label)
    running_loss += (loss - running_loss) / (batch_index + 1)
```

上述计算为 `Simple Moving Average` [Here](https://en.wikipedia.org/wiki/Moving_average)
$$
\bar{p}_{\mathrm{SM}}=\bar{p}_{\mathrm{SM}, \mathrm{prev}}+\frac{1}{n}\left(p_{M}-p_{M-n}\right)
$$
其中 n 为前 n 个数据，这里表示前 n 个 batch



#### squeeze 和 unsqueeze 函数

**squeeze**

用于删除为 1 的维度，以及在某一维度后面增添一个维度

先从 numpy 中建立一个 tensor 如下

```python
import numpy as np
import torch as t
x = np.arange(0,6).reshape(2,1,3)
t = t.from_numpy(x)
print(t)
print(t[0])
print(t[0][0])
print(t.size())
```

其维度为 `[2,1,3]`输出为：

```python
tensor([[[0, 1, 2]],
        [[3, 4, 5]]], dtype=torch.int32)
tensor([[0, 1, 2]], dtype=torch.int32)
tensor([0, 1, 2], dtype=torch.int32)
torch.Size([2, 1, 3])
```

使用 `squeeze` 函数将 1 维度删除

```python
t = t.squeeze()
print(t.size())
```

其输出为

```
torch.Size([2, 3])
```

**Unsqueeze**

建立 tensor ，并使用 `unsqueeze`  在 0 的位置上添加 1 个维度：

```python
x = np.arange(0,6).reshape(2,3)
t = t.from_numpy(x)
print(t)
t = t.unsqueeze(0)
print(t)
print(t.size())
```

其输出如下

```python
tensor([[0, 1, 2],
        [3, 4, 5]], dtype=torch.int32)
tensor([[[0, 1, 2],
         [3, 4, 5]]], dtype=torch.int32)
torch.Size([1, 2, 3])
```



#### Sum 函数

---

在不对维度进行说明是，也就是

```python
x.sum()
```

表示将 x 中所有元素相加

---

`dim = 0`, 表示 按照列相加；`dim = 1` 表示按照行相加

构建 tensor 如下

```python
x = t.rand(2,3)
print(x.size())
print(x)
---
torch.Size([2, 3])
tensor([[0.1344, 0.5664, 0.2169],
        [0.8864, 0.0238, 0.4183]])
```

```python
temx = x.sum(dim = 0)
print(temx)
temx = x.sum(dim = 1)
----
tensor([1.6333, 0.7806, 0.7429])
tensor([1.5036, 1.6985])
```

在多维度的情况下， `a.sim(x)`  表示在 x 表示的**维度位置**上，加和为1

建立 tensor 如下

```python
x = t.rand(2,3,4)
print(x)
temx = x.sum(dim = -1)
print(temx.size())
print(temx)
----
tensor([[[0.3503, 0.9312, 0.0664, 0.3335],
         [0.6146, 0.2542, 0.6631, 0.4043],
         [0.9294, 0.8340, 0.1827, 0.6369]],

        [[0.4219, 0.9337, 0.0449, 0.1226],
         [0.6877, 0.6075, 0.1568, 0.8428],
         [0.2500, 0.8264, 0.3732, 0.2997]]])
torch.Size([2, 3])
tensor([[1.6814, 1.9361, 2.5830],
        [1.5231, 2.2948, 1.7492]])
```

#### Item

对于只存在**一个元素**的 Tensor 进行取值操作

```Python
x = t.rand(1)
print(x)
print(x.item())
----
tensor([0.2352])
0.23517316579818726
```

