---
layout:     post   				    # 使用的布局（不需要改）
title:      Machine Leanring Models Summary 				# 标题 
subtitle:   machine learning           #副标题
date:       2019-10-02 				# 时间
author:     WYX 						# 作者
header-img: img/wuxia4.png 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - decision trees, Bayesian Inference, Navie Bayes

---

---

本文对常见的机器学习模型进行简单总结

---

## 决策树

只要在数据的属性不完全重合的情况下，决策树总能在训练数据上达到 `100%` 的准确度，在此情况下，最后的叶节点多数只有一个数据，所以存在`overfitting`的问题

对于决策树模型，有以下几个问题：

- 如何选取最优的属性进行当前节点的划分
  - 信息增益（Information Gain） $\rightarrow ID3$
  - 信息增益率（Information Gain Ratio） $\rightarrow C4.5$
  - 基尼系数 （Gini）$\rightarrow CART$
- 在节点中数据类别不统一的时候
  - 是继续选择属性划分
    - 有过拟合的风险
  - 还是将当前节点置为叶节点
    - 何时停止属性的划分
- 如何剪枝
  - 预剪枝 (Pre-pruning)
  - 后剪枝 (Post-pruning)

决策树的经典模型分为以下三种：

- 使用信息增益的模型
- C4.5, 使用信息增益率进行当前最优属性的选取
- CART Tree

### 选取最优属性

#### 熵

当前节点的 Entropy 的计算公式如下:
$$
Ent(D) = -\sum _{i=1}^K p_ilog_2p_k
$$
熵衡量了当前节点的 `不确定性`的大小

其中 $p_i$ 为某一类别（如果是二分类，则就是正负类别的数据）占当前节点中**整体数据**数目的比例，K 为当前属性一共具有 K 种可能值

#### 信息增益

下式中，D 为当前节点的数据， $a$ 为当前选取的属性，$|D^v|$ 表示在 a 特征的第 $v$ 个分量上具有的数据数目
$$
Gain(D,a) = Ent(D) - \sum_{v=1} ^V \frac{|D^v|}{|D|}Ent(D^v)
$$


信息增益表示，用属性 $a$ 对当前数据 D 进行划分，获得的增益有多大。因为属性 $a$ 可以有 V 个不同的取值，所以对其进行加权加和

###### 问题

因为熵衡量不确定性的大小，假设当前节点只存在一个数据，则其熵为 
$$
1\times log_2 1 = 0
$$
所以当数据越纯，熵的增益就越大。**信息增益倾向于取值数目较多的属性**。

#### 信息增益率

信息增益率就上述问题进行了限制，计算公式如下：
$$
Gain_ratio(D,a) = \frac{Gain(D,a)}{IV(a)}, \\
IV(a) = -\sum_{v=1}^V \frac{|D^v|}{|D|}log_2\frac{|D^v|}{|D|}
$$
假设 a 有 2 种取值，一共两个数据，则其 $IV(a)$ 为
$$
IV(a) = -2\frac{1}{2}log_2\frac{1}{2}= 1
$$
假设 a 有 3 种取值，一共 3 个数据，则其 $IV(a)$ 为
$$
IV(a) = -3\frac{1}{3}log_2\frac{1}{3}= log_23 > 1
$$
所以，一般若属性 $a$ 的取值数目（V）越多，则其 $IV(a)$ 也会越大

#### 基尼系数

假设样本存在 K 个类别，其中属于第 k 类的概率为 $p_k$ ,则基尼系数为


$$
Gini(D) = \sum_{k=1}^{K} p_k(1-p_k) = 1 - \sum_{k=1}^{K} p_k^2
$$
基尼系数表示：随机抽取两个样本，其不一样的概率。则基尼系数越小，表示样本越纯。则选择属性能使其达到最小基尼指数。根据基尼系数选取最优属性为：
$$
Gini(D,a) =\sum_{v=1}^V \frac{|D^v|}{|D|}Gini(D^v)
$$

### 剪枝

剪枝是为了降低决策树过拟合的风险，分为两种。剪枝的实现需要用到 `Validation set` 

#### 预剪枝 pre-pruning

- 在`training set` 上选取最优的属性之后测得一个准确度
  - 比如选取属性 a 对根节点进行划分，一共分为 $V$ 个子集，在所有的子集上数分类错误的数据，然后算出分类错误的数据占**整体数据**的比例
  - 将 `validation set` 数据同样应用到该树上，以同样的方法测得一个准确度
  - 比较两者的准确度，如果 `validation set` 上准确度较 `training set` 有提升，则保留该属性，否则拒绝该属性划分阿萨德

##### 问题

容易造成 `underfitting` ，有可能一个属性都不会扩展出来。

#### 后剪枝 post-pruning

先将树在 `training set`上完全生长，然后从底向上进行剪枝

- 在 `validation set` 上对这一决策树进行测试，计算出一个准确度
- 从决策树底部选取一个属性，将该属性的划分替换为叶节点
  - 因为替换为叶节点所以，在该节点的判定就按照哪边的数据大就分为哪一类
- 替换为叶节点之后，再次应用 `validation set` 对其进行测试，计算出一个准确度
- 如果准确度有提升，则拿掉该节点
  - 否则保留



### 处理连续值

对于连续值如何选取分割点对当前数据进行切分

- 将某一连续属性值从小到大排序,如下
  $$
  a_1,a_2,a_3,\ldots,a_{V-1},a_V
  $$
  

- 则每次选取 $a_{i-1},a_i$ 之间的中间值，对数据集进行切分。则一共有 n-1 种选择



#### 处理缺失值

对于缺失值的处理，有两种情况

- 当数据的某一属性缺失时，如何进行最优属性的选择？
- 给定了最优属性的选择，当数据的某一属性缺失时，将该数据分到哪一个组？



##### 如何进行最优属性的选择

在有数据缺失的情况下，最优属性的选择只能基于**在该属性下**不缺失的数据，但是同时也要考虑到**在该属性下缺失数据**的多少，则计算公式如下
$$
Gain(D,a) = \frac{D_{未缺失数据}}{D_{全体数据}} Gain(D_{未缺失数据},a)
$$


#### 给定最优属性，如何将缺失属性的数据分组

将缺失属性的数据分入每一个组，但是改变其权重为


$$
\frac{D_{第k属性分值的完整数据}}{D_{全体完整数据}}
$$
假设某一个属性的完整数据为 15 ，全体数据为 16，则有一个数据在此属性上缺失，且该属性有三个分值，三个分值的数据分别为7,5,3

则该缺失数据在三个分值下的权重分别为 $\lbrace \frac{7}{15},\frac{5}{15},\frac{3}{15} \rbrace$, 其他完整数据的权值为 1