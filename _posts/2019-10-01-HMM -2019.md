---
layout:     post   				    # 使用的布局（不需要改）
title:      HMM 				# 标题 
subtitle:   Forward, Backward, EM, Viterbi           #副标题
date:       2019-10-01 				# 时间
author:     WYX 						# 作者
header-img: img/5.8.png 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - HMM

---



## 隐式马尔科夫模型 Hidden Markov Model 

## 各类任务关系图

<center>
    <img src = "https://github.com/joseph-mutu/Pics/raw/master/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%8E%A8%E6%96%AD/HMM4.jpg" width = "400"/>
</center>



在 HMM 中定义了两种概率

- 前向概率 （图中红色）

  - $$
    \alpha_t = P(z_t,x_1,x_2,\ldots,x_t)
    $$

- 后向概率（图中蓝色）

  - $$
    \beta_t = P(z_{t}|x_{t+1},\ldots,x_T)
    $$

    


$$
\left\{
\begin{aligned}

&Learning: \arg\max _Z P(X|\lambda)~ |~ 算法： Baum-Welch(EM)   \\

&Inderence: \left\{
    \begin{aligned}
    & Decoding: \arg\max _Z P(Z|X) ~ |~ 算法： Viterbi    \\
    & Probability of Evidence:  P(X) ~ |~ 算法： Forward | Backward \\
    & Filtering: P(z_t|x_1,\ldots,x_t)  ~ |~ 算法： Forward  \\
    & Smoothing: P(z_t|x_1,\ldots,x_T) ~ |~ 算法： Forward-Backward 联合算法 \\
    & Prediction: \left\{
    	\begin{aligned}
    		& P(z_{t+1}|x_1,\ldots,x_t) ~ |~ 应用了 Filtering 问题 + 齐次马尔科夫假设\\
    		& P(x_{t+1}| x_1,\ldots,x_t) ~ |~ 应用了 P(z_{t+1}|x_1,\ldots,x_t) + 观测独立假设
   		\end{aligned}
   		\right.
    \end{aligned}
    \right.

\end{aligned}
\right.
$$


下面考虑 `Filtering 问题`，`Smoothing 问题`以及 `Prediction 问题`与`前向后向`概率之间的关系

### Filtering 问题

Filtering 问题也可以看做 **Online** 算法，给定到 `t` 时刻的观测数据，求 t 时刻的隐状态
$$
P(z_t|x_1,\ldots,x_t) = \frac{P(x_1,\ldots,x_t,z_t)}{p(x_1,\ldots,x_t)} =  \frac{P(x_1,\ldots,x_t,z_t)}{\sum _{z_t} p(x_1,\ldots,x_t,z_t)}
$$
分母为常数化因子，则 Filtering 问题可以表示为
$$
P(z_t|x_1,\ldots,x_t) \propto P(x_1,\ldots,x_t,z_t) = \alpha_t
$$


### Smoothing 问题

Smoothing 问题可以被视为 Offline 问题，即给定全部数据，预测其中一个数据的隐状态
$$
P(z_t|x_1,\ldots,x_T) = \frac{P(x_{1:t},x_{t+1:T},z_t)}{\sum _{z_t}P(x_1,\ldots,x_T)}
$$
将分子提出来分析:
$$
P(x_{1:t},x_{t+1:T},z_t) = P(x_{t+1:T}|x_{1:t},z_t)\underbrace{P(z_t,x_1,\ldots,x_t)}_{\alpha_t}
$$
上式中的第一项  $P(x_{t+1:T}|x_{1:t},z_t)$

根据概率图模型，因为 从 $x_{1:t}$ 到 $x_{t+1:T}$ 之间一定经过 $z_t,z_{t+1}$,则当 $z_t$ 给定（被观测到），此路径阻断，即


$$
x_{1:t} \perp x_{t+1:T}
$$


则第一项即为


$$
P(x_{1:t},x_{t+1:T},z_t) = \underbrace{P(x_{t+1:T}|z_t)}_{\beta_t}\underbrace{P(z_t,x_1,\ldots,x_t)}_{\alpha_t}
$$
则 Smoothing 解为：
$$
P(z_t|x_1,\ldots,x_T) \propto \alpha_t \beta_t
$$




### Prediction 问题

#### $P(z_{t+1}|x_1,\ldots,x_t)$

$$
P(z_{t+1}|x_1,\ldots,x_t) = \sum_{z_t} P(z_t,z_{t+1}|x_{1:t}) \\
= \sum_{z_t} \underbrace{P(z_{t+1}|x_{1:t},z_t)}_{齐次马尔科夫假设} \underbrace{P(x_{1:t},z_t)}_{filtering 问题} \\
=\sum_{z_t} P(z_{t+1}|z_t) P(x_{1:t},z_t)
$$



#### $P(x_{t+1}| x_1,\ldots,x_t)$

引入隐状态 $x_{t+1}$
$$
P(x_{t+1}| x_1,\ldots,x_t) = \sum _{z_{t+1}} P(z_{t+1},x_{t+1}| x_{1:t}) \\
= \sum _{z_{t+1}} \underbrace{P(x_{t+1}|z_{t+1},x_{1:t})}_{观测独立假设}\underbrace{P(z_{t+1}|x_{1:t})}_{上一个~prediction~问题} \\
= P(x_{t+1}|z_{t+1})P(z_{t+1}|x_{1:t})
$$




----



<center>
    <img src = "https://github.com/joseph-mutu/Pics/raw/master/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%8E%A8%E6%96%AD/HMM1.jpg" width = "300"/>
</center>

---

下述分析中

- 观测值为图上阴影部分，计为 $o_t \in {V_1,\ldots,V_T}$ 也就是 Markov 链长为 **T**
- 隐状态值为图上非阴影部分圆圈，计为 $i_t \in q_1,\ldots,q_N$ ，一共有 **N** 种可能的状态，每一个观测值都有可能从这 N 中状态中产生

----

HMM 模型存在两个假设

- 齐次马尔科夫假设，当前观察状态只与前一个状态有关


$$
P(i_t| i_{t-1},o_{t-1},\ldots, i_1,o_1) = P(i_t|i_{t-1})
$$


- 观测独立性假设，当前观测值仅与当前隐状态有关

$$
P(o_t|i_t,i_{1},\ldots,i_{t-1},o_{1},\ldots,o_{t-1}) = P(o_t|i_t)
$$



### 概率计算

目标为计算 $P(O|\lambda)$

其中 $\lambda$ 为模型参数：初始状态序列 $\pi$，转移概率矩阵 $A$ , 观测概率分布矩阵 $B$

$O$ 为观测概率序列，为 $o_1,o_2,\ldots, o_M$

#### Forward 

<center>
    <img src = "https://github.com/joseph-mutu/Pics/raw/master/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%8E%A8%E6%96%AD/HMM2Forward.png" width = "300"/>
</center>

首先定义前向性概率如下:


$$
\alpha_t(i) = P(o_1,\ldots,o_t,i_t = q_i)
$$
$P(O|\lambda)$ 可以写为


$$
P(O|\lambda) = P(o_1,\ldots,o_T|\lambda) \\
= \sum \limits_{i=1}^N P(o_1,\ldots,o_T,i_t = qi|\lambda) \\
=  \sum \limits_{i=1}^N \alpha_T(q_i)
$$

##### 推导 $\alpha_{t+1}$ 与 $\alpha_t$ 之间的关系

$$
\alpha_{t+1}(j) = P(o_1,\ldots,o_{t+1},i_{t+1} = q_j |\lambda) \\
= \sum \limits_{i=1}^N P(o_1,\ldots,o_{t+1},i_{t+1} = q_j,i_t = q_i|\lambda)
= \sum \limits_{i=1}^N \underbrace{P(o_{t+1}|o_1,\ldots,o_{t},i_{t+1} = q_j,i_t = q_i;\lambda)}_{观测独立性假设} \\ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\times P(o_1,\ldots,o_{t},i_{t+1} = q_j,i_t = q_i|\lambda) \\
=  \sum \limits_{i=1}^N P(o_{t+1}|i_{t+1})P(o_1,\ldots,o_{t},i_{t+1} = q_j,i_t = q_i|\lambda) \\
= \sum \limits_{i=1}^N P(o_{t+1}|i_{t+1})\underbrace{P(i_{t+1} = q_j|o_1,\ldots,o_{t},,i_t = q_i;\lambda)}_{条件独立性假设} \\ ~~~~~~~~~~~~~~~~~~~~~~~~\times P(o_1,\ldots,o_{t},i_t = q_i|\lambda)  \\
= \sum \limits_{i=1}^N P(o_{t+1}|i_{t+1}) P(i_{t+1} = q_j|i_t = q_i;\lambda)\alpha_t(q_i) \\
= \sum \limits_{i=1}^N b_j(o_{t+1}) a_{ij}~\alpha_t(q_i) \\
= b_j(o_{t+1})\sum \limits_{i=1}^N  a_{ij}~\alpha_t(q_i) 
$$

假设 N 为 3，则状态可能的数目为 3，则前向型算法的初始化
$$
\alpha_1(1) = \pi(1)b_1(o_1) \\
\alpha_1(2) = \pi(2)b_2(o_2) \\
\alpha_1(3) = \pi(3)b_3(o_3)
$$




#### Backward

后向算法同样定义了一个后向概率，如下图红色方框所画：

<center>
    <img src = "https://github.com/joseph-mutu/Pics/raw/master/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%8E%A8%E6%96%AD/HMM3Backward.jpg" width = "400"/>
</center>



与前向算法的前向概率（联合概率）不同，后向概率为条件概率

在时刻 t 状态为 $q_i$ 的后向概率为：


$$
\beta_t(i) = P(o_T,\ldots,o_{t+1}|i_t = q_i;\lambda) \\
\vdots \\
\beta_1(i) = P(o_2,\ldots,o_T)|i_t = q_1;\lambda)
$$
则**最终概率**与 $\beta_t(i)$ 之间的关系为:
$$
P(O|\lambda) = P(o_1,\ldots,o_T|\lambda) \\
= \sum \limits_{i=1}^N P(o_1,\ldots,o_T,i_t = q_i|\lambda) \\
=\sum \limits_{i=1}^N P(o_1,\ldots,o_T|i_t = q_i;\lambda)\underbrace{P(i_t=q_i)}_{\pi_i} \\
= \sum \limits_{i=1}^N \underbrace{P(o_1|o_2,\ldots,o_T,i_t = q_i;\lambda)}_{观测独立性假设}\underbrace{P(o_2,\ldots,o_T|i_t=q_i;\lambda)}_{\beta_1(i)}\pi_i \\
= \sum \limits_{i=1}^NP(o_1|i_t = q_i) \beta_1(i) \pi_i \\
= \sum \limits_{i=1}^N b_i(o_1) \beta_1(i) \pi_i
$$

##### 递推公式

$$
\beta_t(i) = P(o_T,\ldots,o_{t+1}|i_t = q_i;\lambda) \\
= \sum \limits_{j=1}^N P(o_T,\ldots,o_{t+1},i_{t+1} = q_j | i_t = q_i;\lambda) \\
= \sum \limits_{j=1}^N \underbrace{P(o_T,\ldots,o_{t+1}|i_{t+1}=q_j,i_t = q_i;\lambda)}_{考虑贝叶斯链式结构的独立性，当 i_t 被观测，则左右节点独立}P(i_{t+1}=q_j|i_t = q_i;\lambda) \\
=\sum \limits_{j=1}^N P(o_T,\ldots,o_{t+1}|i_{t+1} = q_j;\lambda) a_{ij} \\
= \sum \limits_{j=1}^N \underbrace{P(o_{t+1}|o_T,\ldots,o_{t+2},i_{t+1} = q_j;\lambda)}_{观测独立性}\underbrace{P(o_T,\ldots,o_{t+2}|i_{t+1} = q_j;\lambda)}_{\beta_{t+1}(j)} a_{ij} \\
= \sum \limits_{j=1}^N P(o_{t+1}|i_{t+1} = q_j;\lambda) \beta_{t+1}(j) a_{ij}\\
= \sum \limits_{j=1}^N b_j(o_{t+1}) \beta_{t+1}(j) a_{ij}
$$



##### 初始值赋值

假定状态一共有三种可能，则
$$
\beta_T(1) = 1 \\
\beta_T(2) = 1\\
\beta_T(3) = 1
$$

### 模型参数估计 EM 算法

给定观测序列，估计模型的参数，使得观测序列的概率 $P(O|\lambda)$  最大

在 HMM 中存在 3 个 参数，$\lambda = \lbrace \pi,\textbf{A},\textbf{B}  \rbrace$  

观测序列的概率 $P(O|\lambda)$ 可以写为：


$$
P(O|\lambda) = \sum _I P(O,I|\lambda) \\
= \sum _{i_1}^N \sum _{i_2}^N\ldots \sum _{i_T}^N \pi_{i_1} \prod_{t=2}^N a_{i_{t-1},i_t} \prod_{t=1}^N b_{i_{t}}(o_t) \tag{1}
$$


回忆 EM 算法参数更新的公式
$$
\theta^{t+1} = \int_Z P(X,Z|\theta) P(Z|X;\theta^{t}) dz \tag{2}
$$


将对应的参数换到 HMM 的模型下，其中


$$
X : 观测变量 : O\\
Z: 隐变量 : I\\
\theta: 模型参数 : \lambda
$$
则 (1) 式可以写为
$$
\lambda^{t+1} = \arg \max _\pi \int _I logP(O,I|\lambda) \underbrace{P(I|O;\lambda^{t})} dI \tag{3}
$$
先对上式第二项进行改写

**注意**： EM 算法为迭代算法，$\lambda^t$ 为上一轮的迭代过程的参数值，可以被视为常量


$$
P(I|O;\lambda^{t}) = \frac{P(I,O|\lambda^t)}{P(O|\lambda^{t})}
$$


因为 O 为给定观测序列，且 $\lambda^t$ 为常量，则 $P(O|\lambda^t)$ 为常量，不影响 $\arg max $ 的结果，则 (3) 式可以写为


$$
\lambda^{t+1} = \arg \max _\pi\int _I logP(O,I|\lambda) P(I,O|\lambda^t) \tag{4} dI
$$
将 (1) 式子的联合概率带入 (4) 式可以得到


$$
\lambda^{t+1} = \arg\max_\pi \sum _I log P(O,I|\lambda) P(I,O|\lambda^t)
$$


定义 Q 函数
$$
Q(\lambda,\lambda^t) = \sum _I logP(O,I|\lambda) P(I,O|\lambda^t) \\
= \sum _I [ (log\pi_{i_1} + \sum_{t=2}^N loga_{i_{t-1},i_t} + \sum_{t=1}^N b_{i_{t}}(o_t)~) P(I,O|\lambda^t)]
$$


#### 求 $\pi^{t+1}$

$$
\pi^{t+1} = \arg\max_\pi Q(\lambda,\lambda^t) \\
=\arg\max_\pi\sum _I [ (log\pi_{i_1} + \underbrace{\sum_{t=2}^N loga_{i_{t-1},i_t} + \sum_{t=1}^N b_{i_{t}}(o_t)~)}_{与 \pi 无关} P(I,O|\lambda^t)] \\
=\arg\max_\pi \sum _I~ log\pi_{i_1} ~P(I,O|\lambda^t)\\
= \arg\max_\pi\sum_{i_1}\sum_{i_2}\ldots \sum_{i_T} [~ log\pi_{i_1} ~P(i_1,\underbrace{i_2,\ldots,i_T}_{与 i_1无关},O|\lambda^t)] \\
=\arg\max_\pi  \sum_{i_1} [log \pi_{i_1} P(i_1,O|\lambda^t)] \\
= \arg\max_\pi  \sum_{i=1}^N [log \pi_{i} P(i_1 = q_i,O|\lambda^t)] \tag{5}
$$



注意上述  (5) 式子中的 优化问题存在约束
$$
\sum_{i=1}^N \pi_i = 1
$$
则根据约束建立拉格朗日函数


$$
\mathcal{L} =  \sum_{i=1}^N [log \pi_{i} P(i_1 = q_i,O|\lambda^t)] + \eta(\sum_{i=1}^N \pi_i - 1) \tag{6}
$$
对 (6) 式子求关于 $\pi_i$ 的导数，


$$
\frac{\partial \mathcal{L}}{\partial \pi_i} \Rightarrow \frac{1}{\pi_i} P(i_1 = q_i,O|\lambda^t) + \eta = 0 \\
\Rightarrow P(i_1 = q_i,O|\lambda^t) + \pi_i\eta = 0 \\
引入约束\\
\Rightarrow \sum_{i=1}^N P(i_1 = q_i,O|\lambda^t) + \pi_i\eta =0 \\
\Rightarrow P(O|\lambda^t) + \eta = 0\\
\Rightarrow \eta = -P(O|\lambda^t)
$$
将 $\eta $ 的求值带回 (6) 式中求解 $ \pi_i $,如下
$$
\frac{\partial \mathcal{L}}{\partial \pi_i} \Rightarrow \frac{1}{\pi_i} P(i_1 = q_i,O|\lambda^t) + -P(O|\lambda^t) = 0 \\
\Rightarrow \pi_i = \frac{P(i_1 = q_i,O|\lambda^t)}{P(O|\lambda^t)}
$$
