---
layout:     post   				    # 使用的布局（不需要改）
title:      CS 224N Assignment 				# 标题 
subtitle:   nlp         #副标题
date:       2019-08-13 				# 时间
author:     WYX 						# 作者
header-img: img/1.3.png 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - CS224N, NLP


---



# CS 224N Assignment 



## Assignment 2

### Written: Understanding word2vec

#### a



$y_w$ 是真实的 one hot 标签。是一个只有一个值为 1 其他值均为 0 的 one hot 向量。所以


$$
- \sum \limits_{w\in Vocab} y_w log(\hat{y_w}) = -log (\hat{y_w})
$$

#### b 计算 $\frac{\partial J}{\partial v_c}$



Word2Vec 算法的概率值计算如下



$$
P(O = o | C =c) = \frac{exp(u_o^T v_c)}{\sum \limits_{w\in Vocab} exp(u_w^T v_c)}
$$



**Note:** 此处 O 代表预测单词，C 代表 center 单词。 **u** 向量为 out 向量矩阵（第二个矩阵）中的某一列，**v** 向量为 in 向量矩阵（第一个矩阵）中的某一行。

 Word2vec 的目标函数如下：
$$
J = -log P(O=o|C = c) \\
\space \\
= - log~exp(u_o^T v_c) + log\sum \limits_{w \in Vocab} ~ exp(u_w^T v_c)
$$


为了是链式法则更加清晰，现将目标函数的各个分量定义如下:


$$
s_c = u_o^T v_c \\
\space  \\
\hat{y}_w = \frac{exp~s_c}{\sum \limits_{w \in Vocab}exp~ s_w}
$$


则 


$$
J = log \sum \limits_{w \in Vocab} exp(s_w) - s_c
$$


计算偏微分


$$
\frac{\partial J}{\partial v_c} = \frac{\partial J }{\partial s_c} \frac{\partial s_c}{\partial v_c}
$$
在链式法则中，各个分项的具体表达式为:


$$
\frac{\partial J}{\partial s_c} = \hat{y}_c - y_c \\
\space \\
\frac{\partial s_c}{\partial v_c} = u_o
$$

---

**Note**: 对 $\log \sum \limits_{w \in Vocab} exp s_w$ 求导的结果为  $\hat{y}_c$
$$
\frac {\partial~ log x}{\partial x} = \frac{1}{x} \\
\space \\
\frac{\partial exp~ x}{\partial x} = exp ~x
$$

---

则 


$$
\frac{\partial J}{\partial v_c} = (\hat{y}_c - y_c) \textbf{U}_o
$$


#### c 计算 $\frac{\partial J }{\partial u_w}$


$$
J = log \sum \limits_{w \in Vocab} exp(s_w) - s_c \\
\space \\
J = log \sum \limits_{w \in Vocab} exp(u_w^T v_c) - u_o^T v_c
$$
对 **U** 矩阵求导分两种情况，当 $w = o$ 时；当 $w \neq o$ 时。此处 w 和 o 均为 index	


$$
\frac{\partial J}{\partial u_o} = (\hat{y}_c  - 1)v_c^T, when ~ o = w \\
\space \\
\frac{\partial J}{\partial u_w} = \hat{y}_w v_c,  when ~ o \neq w
$$


#### d 计算 sigmoid 函数的偏导数


$$
\delta(x) = \frac{1}{1 + e^{-x}} \\
\space \\
\Rightarrow \frac{\partial \delta}{\partial x} = \frac{e^{-x}}{(1+e^{-x})^2} = \frac{e^{-x}}{1+e^{-x}} \frac{1}{1+e^{-x}} = \frac{1}{1+e^{x}}\frac{1}{1+e^{-x}} = (1-\delta(x))\delta(x)\\
\space \\
\textbf{Note:}~\frac{1}{1+e^x} : \frac{1+e^{-x}}{1+e^{-x}} - \frac{1}{1+e^{-x}} = \frac{e^{-x}}{1+e^{-x}} = \frac{1}{1+e^x}
$$


#### e Negative Sampling 负采样



负采样目标函数如下：


$$
J_{neg} = -log(\delta(u_o^T v_c)) - \sum_{k=1}^K log(\delta(-u_k^Tv_c)) \\
\space \\
J_{neg} = -log (\delta(x)) - \sum_{x} log (\delta(x))
$$
其中 $w_1,w_2,\ldots,w_k$ 为采取的负样本。且 $o ∉ \lbrace w_1,w_2,\ldots,w_k \rbrace$



###### 求 $\frac{\partial J_{neg}}{\partial v_c}$

##### 

$$
\frac{\partial J_{neg}}{\partial v_c} = \frac{\partial J_{neg}}{\partial \delta(x)}\frac{\partial \delta(x)}{\partial x} \frac{\partial x}{\partial v_c}  \\
\space \\
\Rightarrow \frac{1}{\delta(u_o^T v_c)}\cdot \delta(u_o^T v_c)(1-\delta(u_o^T v_c))\cdot u_o - \sum_k \lbrace (1-\delta(u_k^T v_c)) u_k \rbrace
$$



###### 求 $\frac{\partial J_{neg}}{\partial u_o }$

Note：$o \notin \lbrace w_1,w_2,\ldots,w_k \rbrace$
$$
\frac{\partial J_{neg}}{\partial u_o} = \frac{\partial J_{neg}}{\partial \delta(x)} \frac{\partial \delta(x)}{\partial x} \frac{\partial x}{\partial u_o} \\
\space \\
\Rightarrow \frac{1}{\delta(u_0^Tv_c)}\cdot \delta(u_o^T v_c)(1-\delta(u_o^T v_c))\cdot v_c^T \\
\space \\
\Rightarrow (1-\delta(u_o^T v_c)) v_c^T
$$


###### 求 $\frac{\partial J_{neg}}{\partial u_k }$


$$
\frac{\partial J_{neg}}{\partial u_k} = \frac{\partial J_{neg}}{\partial \delta(x)} \frac{\partial \delta(x)}{\partial x} \frac{\partial x}{\partial u_k} \\
\space \\
\Rightarrow  (1-\delta(u_k^T v_c)) v_c 
$$


###### 为什么负采样比最初的目标函数要更加高效

- 仅使用了部分“负样例”
- 计算 sigmoid 函数而不是 softmax 函数



#### 写出窗口的偏微分函数


$$
\frac{\partial J_{skip-gram}(v_c,w_{t-m},\ldots,w_{t+m},\textbf{U})}{\partial \textbf{U}} = \sum \limits_{-m<=j<=m,j\neq0} \frac{J(v_c,	w_{t+j},\textbf{U})}{\partial \textbf{U}} \\
\space \\
\frac{\partial J_{skip-gram}(v_c,w_{t-m},\ldots,w_{t+m},\textbf{U})}{\partial v_c} = \sum \limits_{-m<=j<=m,j\neq0} \frac{J(v_c,	w_{t+j},\textbf{U})}{\partial v_c} \\
\space \\
\frac{\partial J_{skip-gram}(v_c,w_{t-m},\ldots,w_{t+m},\textbf{U})}{\partial v_w} = 0
$$
