---
layout:     post   				    # 使用的布局（不需要改）
title:      CS 224N Assignment 				# 标题 
subtitle:   nlp         #副标题
date:       2019-08-13 				# 时间
author:     WYX 						# 作者
header-img: img/1.3.png 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - CS224N, NLP


---



# CS 224N Assignment 

## Assignment 2

### Written: Understanding word2vec

#### a



$y_w$ 是真实的 one hot 标签。是一个只有一个值为 1 其他值均为 0 的 one hot 向量。所以


$$
- \sum \limits_{w\in Vocab} y_w log(\hat{y_w}) = -log (\hat{y_w})
$$






#### b 计算 $\frac{\partial J}{\partial v_c}$



Word2Vec 算法的概率值计算如下



$$
P(O = o | C =c) = \frac{exp(u_o^T v_c)}{\sum \limits_{w\in Vocab} exp(u_w^T v_c)}
$$



**Note:** 此处 O 代表预测单词，C 代表 center 单词。 **u** 向量为 out 向量矩阵（第二个矩阵）中的某一列，**v** 向量为 in 向量矩阵（第一个矩阵）中的某一行。

 Word2vec 的目标函数如下：
$$
J = -log P(O=o|C = c) \\
\space \\
= - log~exp(u_o^T v_c) + log\sum \limits_{w \in Vocab} ~ exp(u_w^T v_c)
$$


为了是链式法则更加清晰，现将目标函数的各个分量定义如下:


$$
s_c = u_o^T v_c \\
\space  \\
\hat{y}_w = \frac{exp~s_c}{\sum \limits_{w \in Vocab}exp~ s_w}
$$


则 


$$
J = log \sum \limits_{w \in Vocab} exp(s_w) - s_c
$$


计算偏微分


$$
\frac{\partial J}{\partial v_c} = \frac{\partial J }{\partial s_c} \frac{\partial s_c}{\partial v_c}
$$
在链式法则中，各个分项的具体表达式为:


$$
\frac{\partial J}{\partial s_c} = \hat{y}_c - y_c \\
\space \\
\frac{\partial s_c}{\partial v_c} = u_o
$$

---

**Note**: 对 $\log \sum \limits_{w \in Vocab} exp s_w$ 求导的结果为  $\hat{y}_c$
$$
\frac {\partial~ log x}{\partial x} = \frac{1}{x} \\
\space \\
\frac{\partial exp~ x}{\partial x} = exp ~x
$$

---

则 


$$
\frac{\partial J}{\partial v_c} = (\hat{y} - y) \textbf{U}
$$


#### c 计算 $\frac{\partial J }{\partial u_w}$


$$
J = log \sum \limits_{w \in Vocab} exp(s_w) - s_c \\
\space \\
J = log \sum \limits_{w \in Vocab} exp(u_w^T v_c) - u_o^T v_c
$$
对 **U** 矩阵求导分两种情况，当 $w = o$ 时；当 $w \neq o$ 时。此处 w 和 o 均为 index	


$$
\frac{\partial J}{\partial u_o} = (\hat{y}_c  - 1)v_c^T, when ~ o = w \\
\space \\
\frac{\partial J}{\partial u_w} = \hat{y}_w v_c,  when ~ o \neq w
$$


#### d 计算 sigmoid 函数的偏导数


$$
\delta(x) = \frac{1}{1 + e^{-x}} \\
\space \\
\Rightarrow \frac{\partial \delta}{\partial x} = \frac{e^{-x}}{(1+e^{-x})^2} = \frac{e^{-x}}{1+e^{-x}} \frac{1}{1+e^{-x}} = \frac{1}{1+e^{x}}\frac{1}{1+e^{-x}} = (1-\delta(x))\delta(x)\\
\space \\
\textbf{Note:}~\frac{1}{1+e^x} : \frac{1+e^{-x}}{1+e^{-x}} - \frac{1}{1+e^{-x}} = \frac{e^{-x}}{1+e^{-x}} = \frac{1}{1+e^x}
$$


#### e Negative Sampling 负采样



负采样目标函数如下：


$$
J_{neg} = -log(\delta(u_o^T v_c)) - \sum_{k=1}^K log(\delta(-u_k^Tv_c)) \\
\space \\
J_{neg} = -log (\delta(x)) - \sum_{x} log (\delta(-x))
$$
其中 $w_1,w_2,\ldots,w_k$ 为采取的负样本。且 $o ∉ \lbrace w_1,w_2,\ldots,w_k \rbrace$



###### 求 $\frac{\partial J_{neg}}{\partial v_c}$

 

$$
\frac{\partial J_{neg}}{\partial v_c} = \frac{\partial J_{neg}}{\partial \delta(x)}\frac{\partial \delta(x)}{\partial x} \frac{\partial x}{\partial v_c}  \\
\space \\
\Rightarrow -\frac{1}{\delta(u_o^T v_c)}\cdot \delta(u_o^T v_c)(1-\delta(u_o^T v_c))\cdot u_o - \sum_k \lbrace (1-\delta(u_k^T v_c)) u_k \rbrace \\
\space \\
\Rightarrow -(1-\delta(u_o^T v_c))\cdot u_o - \sum_k \lbrace (1-\delta(u_k^T v_c)) u_k \rbrace
$$



###### 求 $\frac{\partial J_{neg}}{\partial u_o }$

Note：$o \notin \lbrace w_1,w_2,\ldots,w_k \rbrace$
$$
\frac{\partial J_{neg}}{\partial u_o} = \frac{\partial J_{neg}}{\partial \delta(x)} \frac{\partial \delta(x)}{\partial x} \frac{\partial x}{\partial u_o} \\
\space \\
\Rightarrow \frac{1}{\delta(u_0^Tv_c)}\cdot \delta(u_o^T v_c)(1-\delta(u_o^T v_c))\cdot v_c^T \\
\space \\
\Rightarrow (1-\delta(u_o^T v_c)) v_c^T
$$


###### 求 $\frac{\partial J_{neg}}{\partial u_k }$


$$
\frac{\partial J_{neg}}{\partial u_k} = \frac{\partial J_{neg}}{\partial \delta(x)} \frac{\partial \delta(x)}{\partial x} \frac{\partial x}{\partial u_k} \\
\space \\
\Rightarrow  (1-\delta(u_k^T v_c)) v_c 
$$


###### 为什么负采样比最初的目标函数要更加高效

- 仅使用了部分“负样例”
- 计算 sigmoid 函数而不是 softmax 函数



#### 写出窗口的偏微分函数


$$
\frac{\partial J_{skip-gram}(v_c,w_{t-m},\ldots,w_{t+m},\textbf{U})}{\partial \textbf{U}} = \sum \limits_{-m<=j<=m,j\neq0} \frac{J(v_c,	w_{t+j},\textbf{U})}{\partial \textbf{U}} \\
\space \\
\frac{\partial J_{skip-gram}(v_c,w_{t-m},\ldots,w_{t+m},\textbf{U})}{\partial v_c} = \sum \limits_{-m<=j<=m,j\neq0} \frac{J(v_c,	w_{t+j},\textbf{U})}{\partial v_c} \\
\space \\
\frac{\partial J_{skip-gram}(v_c,w_{t-m},\ldots,w_{t+m},\textbf{U})}{\partial v_w} = 0
$$





## Assignment 3 

### Machine Learning & Neural Networks 

#### a.  Adam Optimizer

1. 动量下降法，应用了加权平均的想法（weighted average），中和了 SGD 方法产生的多余的波动。具体可以查看[这篇文章]([https://terrifyzhao.github.io/2018/02/16/%E5%8A%A8%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95Momentum.html](https://terrifyzhao.github.io/2018/02/16/动量梯度下降法Momentum.html))





### Neural Transition-Based Dependency Parsing

#### a) 

<center>
    <img src = "https://github.com/joseph-mutu/Pics/raw/master/CS224n%20Parsing%20sentence.jpg" width = "400"/>
</center>



Note: 这个 LEFT-ARC 与 RIGHT-ARC 描述了 **被修饰者** 与 **修饰者**之间的关系。箭头总是从**被修饰者**指向**修饰者**。 且被修饰者被称为 head，修饰者被称为 dependent。每次消去时，消去 **dependent** 。

- I parsed ： I 修饰 parsed。则箭头为 I$\leftarrow$ parsed

- This sentence: this 修饰 sentence，则箭头为 this $\rightarrow$ sentence

  

| Stack                       | Buffer                                 | New dependency                | Transition            |
| --------------------------- | -------------------------------------- | ----------------------------- | --------------------- |
| [ROOT]                      | [I, parsed, this, sentence, correctly] |                               | Initial Configuration |
| [ROOT, I]                   | [parsed, this, sentence, correctly]    |                               | SHIFT                 |
| [ROOT, I, parsed]           | [this, sentence, correctly]            |                               | SHIFT                 |
| [ROOT, parsed]              | [this, sentence, correctly]            | parsed $\rightarrow$ I        | LEFT-ARC              |
| [ROOT,parsed,this]          | [sentence, correctly]                  |                               | SHIFT                 |
| [ROOT,parsed,this,sentence] | [correctly]                            | this $\rightarrow$ sentence   | LEFT-ARC              |
| [ROOT,parsed,sentence]      | [correctly]                            | parsed $\leftarrow$ sentence  | RIGHT-ARC             |
| [ROOT,parsed,correctly]     | []                                     | parsed $\leftarrow$ correctly | RIGHT-ARC             |
| [ROOT,parsed]               | []                                     | ROOT $\leftarrow$ parsed      | LEFT-ARC              |
| [ROOT]                      | []                                     | []                            | DONE                  |



#### b) A sentence containing n words will be parsed in how many steps (in terms of n)? Briefly explain why.

- 每一个单词需要先进行 SHIFT 操作，移进 Stack。
- 每个单词至少需要进行一次 ARC 操作，进行消去

所以总数为 2n