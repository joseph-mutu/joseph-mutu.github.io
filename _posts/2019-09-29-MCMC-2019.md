---
layout:     post   				    # 使用的布局（不需要改）
title:      MCMC 				# 标题 
subtitle:   Markov Chain and Monte Carlo          #副标题
date:       2019-09-29 				# 时间
author:     WYX 						# 作者
header-img: img/1.6.png 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - MCMC

---



## 蒙特卡洛方法 Monte Carlo Method 



$$
Inference \Rightarrow 
\begin{cases}
精确推断\\
近似推断 \begin{cases}
确定性 &\Rightarrow Variational Inference \\
随机 &\Rightarrow MCMC
\end{cases}
\end{cases}
$$



本文MH算法伪码截图出自 “LDA 数学八卦”

---

### 目标

蒙特卡洛方法意在解决复杂函数的期望，如下


$$
E_{z|x}[f(z)] = \int p(z|x)f(z) dz \tag{1}
$$


因为 f(z) 函数以及 p(z|x) 函数过于复杂或者无法找到显式的表达式，所以使得积分难以求解

如果可以获得 $z_1,z_2,\ldots,z_N \sim p(z|x)$  服从后验概率密度函数的 N 个样本点，则上式  (1) 可以使用 （2） 近似


$$
E_{z|x}[f(z)] = \int p(z|x)f(z) dz \approx \frac{1}{N}\sum \limits_{i=1}^N f(z_i) \tag{2}
$$

#### 如何从复杂分布中采样

以下不对具体方法进行重复，只说明理解过程中遇到的问题

##### 根据马氏链的收敛性进行采样

已知**特定**的概率转移矩阵 **P** 存在以下性质
$$
\lim \limits_{n\rightarrow \infty} P^{n} = \pi(j)
$$
其中 $\pi(j)$ 为平稳分布，平稳分布与初始状态的选取无关，只与特定的矩阵 **P** 相关

**马氏链每一个节点对应一个分布（也就是一个状态），每一个状态都会别抽取一个样本，也就是每个节点都会有相应的样本**

假设初始概率分布如下
$$
[0.3,0.4,0.3]
$$
则初始状态为根据上述概率分布选取的一个样本$\in{1,2,3}$ 分别对应上述 0.3,0.4,0.3 的概率，假设初始状态为 **1**


$$
\pi(2) = \pi(1) \times \textbf{P} \tag{3}
$$
上式对应一次马氏链状态的跳跃。

但是，在状态 2下抽取样本，要根据其**相应转移矩阵的概率**进行采样，也就是 **P(状态2|X=1)** 。马氏链每进行一次状态的转移，都要根据前一个样本的状态对**相应的转移矩阵的概率**进行采样


$$
\pi(3) = \pi(2)\times \textbf{P} \\
\space \\
=\pi(1) \times \textbf{P}^2
$$
**则 $P$** 矩阵每一次自乘就是一次状态的更新。状态2下的第二个样本要根据 $P(x_1,x_2,x_3| X = 1)$ 进行抽样。则该样本算作从 $\pi(2)$ 进行的抽样

每一次状态更新后，新的状态计算如下

则如果当前状态值为 1 的话，在新状态下，只需要沿着 $ P(p_{11}|oldX_1),P(p_{12}|oldX_1),P(p_{13}|oldX_1)$ 采样即可
$$
P(newX_1) = P(oldX_1) \times P(p_{11}|oldX_1) + P(oldX_2) \times P(p_{21}|oldX_2) + P(oldX_3) \times P(p_{31}|oldX_3)\\
\space \\
P(newX_2) = P(oldX_1) \times P(p_{12}|oldX_1) + P(oldX_2) \times P(p_{22}|oldX_2) + P(oldX_3) \times P(p_{32}|oldX_3)\\
\space \\
P(newX_3) = P(oldX_1) \times P(p_{13}|oldX_1) + P(oldX_2) \times P(p_{23}|oldX_2) + P(oldX_3) \times P(p_{33}|oldX_3)
$$



$$
\textbf{P} = \left\{
\begin{matrix}
0.9 & 0.05 & 0.05\\
0.15 & 0.8 & 0.05 \\
0.25 & 0.25 & 0.5
\end{matrix}
\right\}_{3x3}
$$



##### 构建概率转移矩阵

根据`细致平稳分布条件`,分布与概率矩阵 **P** 满足以下条件时，该分布为平稳分布：


$$
\pi^m(j) \textbf{P}_{ji} = \pi^m(i)\textbf{P}_{ij} \tag{4}
$$
Note: $\pi^m$ 表示当前状态为 第 m 次的状态，下文以 **q(i,j)** 表示概率转移矩阵中的值

但是一般情况下， (4) 式并不成立，为了使`细致平稳条件`达成，引入变量 $\alpha(i,j)$ ，将 (4) 式改写为


$$
p(j)q(j,i) \alpha(j,i) = p(i)q(i,j) \alpha(i,j) \tag{5}
$$


将 (5) 式中的 $\alpha$ 根据对称性进行改写，如下，则细致平稳条件得到满足


$$
p(j)q(j,i) \underbrace{p(i)q(i,j)}_{\alpha(j,i)} = p(i)q(i,j)\underbrace{p(j)q(j,i)}_{\alpha(i,j)} \tag{6}
$$


则新的概率转移矩阵为 $q(i,j) \times \alpha(i,j)$ ，称 $\alpha(i,j)$ 为接受率



<center>
    <img src = "https://github.com/joseph-mutu/Pics/raw/master/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%8E%A8%E6%96%AD/MCMC1.jpg" width = "700"/>
</center>



##### 接受率太小 -- MH 采样

假设 (6) 式中的 $\alpha(i,j) = 0.1$  以及 $\alpha(j,i) = 0.2$ 很小，则根据上面描述的算法，从均匀分布中随机选取的数字一直 大于 $\alpha(x_t,y)$ ,则样本一直不接受转移

MH算法则是解决了 $\alpha(i,j)$ 接受率太小的问题，将细致平稳条件两端同步放大


$$
p(j)q(j,i)\times \underbrace{0.1}_{p(i)q(i,j)} = p(i)q(i,j) \times \underbrace{0.2}_{p(j)q(j,i)} \tag{6}
$$
因为 $\alpha(i,j)$ 为 <=1 的数字，则利用下式


$$
\alpha(i,j) = min \lbrace \frac{p(i)q(i,j)}{p(j)q(j,i)},1 \rbrace
$$
则 (6) 式可以变为


$$
p(j)q(j,i)\times \frac{0.1}{0.2} = p(i)q(i,j) \times \frac{0.2}{0.1} \\
\space \\
p(j)q(j,i)\times 0.5 = p(i)q(i,j) \times 1 ~
$$

<center>
    <img src = "https://github.com/joseph-mutu/Pics/raw/master/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%8E%A8%E6%96%AD/MCMC2.jpg" width = "700"/>
</center>



#### Gibbs 采样

假设存在二维平稳分布 **p(x,y) ** ,取两个点 $A(x_1,y_1)$ ,$B(x_1,y_2)$ 即两个点均位于 $x_1$ 这一坐标轴



<center>
    <img src = "https://github.com/joseph-mutu/Pics/raw/master/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%8E%A8%E6%96%AD/Gibbs3.jpg" width = "400"/>
</center>



由下式
$$
p(x_1,y_1) p(y_2|x_1) = p(x_1) p(y_1|x_1) p(y_2|x_1) \\
\space \\
= p(x_1,y_2) p(y_2|x_1) = p(x_1) p(y_2|x_1) p(y_2|x_1)
$$


可以得到，由 **真实分布的条件概率** 作为概率转移矩阵，则该**概率转移矩阵最终会收敛为真实分布**



<center>
    <img src = "https://github.com/joseph-mutu/Pics/raw/master/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%8E%A8%E6%96%AD/Gibbs1.jpg" width = "700"/>
</center>



Note：在 Gibbs 采样中，每次只针对一个坐标轴进行采样，也就是相对于上一个样本 ($x_t,y_t$) 而言，下一个样本只改变其中一个坐标轴的值

从算法 7 中得到的样本为


$$
(x_0,y_0) , (y_1,x_0),(y_1,x_1),(y_2,x_1),\ldots (x_t,y_{t+1}),(x_{t+1},y_{t+1})
$$
注意，上述的算法 Gibbs 采样 和 MH 采样，都只是保证构建的构建的概率矩阵**最终会收敛为 真实的分布**，但是初始状态仍然是随机选择的，所以仍然需要 `burn-in` 的处理，经过一定次数的迭代之后，认为当前分布趋近于真实分布

下述为 n 维的 Gibbs 采样



<center>
    <img src = "https://github.com/joseph-mutu/Pics/raw/master/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%8E%A8%E6%96%AD/Gibbs2.jpg" width = "700"/>
</center>



