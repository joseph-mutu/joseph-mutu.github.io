---
layout:     post   				    # 使用的布局（不需要改）
title:      GMM 				# 标题 
subtitle:   GMM,MLE,EM #副标题
date:       2019-04-14 				# 时间
author:     WYX 						# 作者
header-img: img/1.2.png 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - PGM,EM,GMM,MLE
---



# GMM 高斯混合模型

### 离散变量边缘分布

从几何角度来看，一个样本采样的过程为多个高斯分布的叠加合成:


$$
p(x) = \sum_{k=1}^K \alpha_k N(x|\mu_k,\delta_k^2),\sum_{k=1}^K\alpha_k = 1
$$
![Gaussian Model](https://pic.superbed.cn/item/5cb582273a213b041754f585)



从混合模型的角度来看

* X 为 observed data $\rightarrow​$ X = {$x_1,x_2,...,x_n​$}
* Z 为 latent variable
* $\sum_{k=1}^{K}p_k = 1​$



对于每一个样本，都存在下表，表示某一个样本属于不同的高斯分布的概率

|   Z   | $C_1$ | $C_2$ | $...$ | $C_N$ |
| :---: | :---: | :---: | :---: | :---: |
| $p_K$ | $p_1$ | $p_2$ | $...$ | $p_n$ |



某一个样本 $x$ ，它既有可能属于 $c_1$高斯分布，也有可能属于$c_2$高斯分布，但是属于$c_1$ 高斯分布的概率大一点



### 概率密度函数

一个样本的概率密度函数 $p(k)$可以表示为:



$$
p(x) = \sum_Z P(X,Z) \\
= \sum_{k=1}^K P(X,Z = C_k) \\
=  \sum_{k=1}^K P(Z = C_k) P(X|Z = C_k) \\
= \sum_{k=1}^K p_k N(x|\mu_k,\delta_k^2)
$$


## 生成过程

假设有一个 k 面的**不均匀骰子**,每一面出现的概率对应上表中的$p_k$

1. 投骰子，选择一个面，假设为 $p_k$
2. 选中为 $p_k$ 的高斯分布
3. 从$p_k​$的高斯分布中采样，得到一个样本

 

## 为什么极大似然估计不行

对于一个样本，其参数为



* $\theta:​$ {$p_1,p_2,...,p_k;\mu_1,\mu_2,...,\mu_k;\delta_1,\delta_2,...,\delta_n​$}



极大似然过程可以表示为:



$$
\hat\theta_{MLE} = arg\max_{\theta} log\prod_i^Np(x_i) \\
 = arg\max_{\theta} \sum_i^N log p(x_i) \\
 = arg\max_{\theta} \sum_i^N log \space \underbrace {(\sum_{k=1}^K p_kN(x_i|\mu_k,\delta_k^2))}_{log(f_1+f_2+f_3+...+f_K)}
$$





其中单个高斯分布的概率密度函数可以表示为


$$
f_k = p_k N(x_i|\mu_k,\delta_k^2) = p_k \frac{1}{\sqrt{2\pi} \delta_k} exp(- \frac{(x_i-\mu_k)^2}{2\delta_k^2})
$$



上述过程无解析解



### 引入 EM 算法求解 GMM 模型

* X: observed data $\rightarrow$X = {$x_1,x_2,...,x_n$}

* (X,Z): complete data $\rightarrow$$(x_1,z_1),(x_2,z_2),(x_3,z_3),...,(x_n,z_n)$



EM 算法一般形式表达


$$
\theta^{t+1} = arg\max_\theta \underbrace{ \mathbb{E}_{Z|X,\theta^t}[logP(X,Z|\theta)]}_{Q(\theta,\theta^t)}
$$

$$
\theta^{t+1} = arg\max_\theta Q(\theta,\theta^t)
$$



将 Q 函数进行展开:



$$
Q(\theta,\theta^t) = \int_Z P(Z|X,\theta^t) logP(X,Z|\theta) dz \\
= \sum_Z P(Z|X,\theta^t) logP(X,Z|\theta) \\
$$



在原本假设中，数据的采样为独立同分布$iid$



$$
= \sum_Z  log\prod_i^NP(x_i,z_i|\theta) \prod_i^N P(z_i|x_i,\theta^t) \\
= \sum_{z_1,z_2,...,z_n} \underbrace{\sum_i^N logP(x_i,z_i|\theta)}_{展开} \prod_i^N P(z_i|x_i,\theta^t)
$$

$$
=\sum_{z_1,z_2,...,z_n} [\underbrace{logP(x_1,z_1|\theta)}_{提取第一项}+logP(x_2,z_2|\theta)+...+logP(x_n,z_n|\theta)] \prod_i^N P(z_i|x_i,\theta^t)
$$

---



将第一项提取，进行分析


$$
\Longrightarrow \sum_{z_1,z_2,...,z_n}logP(x_1,z_1|\theta)\prod_i^N P(z_i|x_i,\theta^t)\\
\Longrightarrow \sum_{z_1,z_2,...,z_n}\underbrace{logP(x_1,z_1|\theta)}_{只与z_1 相关}\space[P(z_1|x_1,\theta^t) P(z_2|x_2,\theta^t)...P(z_n|x_n,\theta^t)]
$$


提取乘积的首项


$$
\Longrightarrow \sum_{z_1}logP(x_1,z_1|\theta) P(z_1|x_1,\theta^t) \underbrace{\sum_{z_2,z_3,...,z_n}[P(z_2|x_2,\theta^t)...P(z_n|x_n,\theta^t)]}_1 \tag{1}
$$

$$
\Longrightarrow \sum_{z_1}logP(x_1,z_1|\theta) P(z_1|x_1,\theta^t) \sum_{z_2,z_3,...,z_n}P(z_2|x_2,\theta^t) \sum_{z_3,z_3,...,z_n}P(z_3|x_3,\theta^t)...\sum_{z_2,z_3,...,z_n}P(z_n|x_n,\theta^t) \tag{2}
$$

$$
\Longrightarrow \sum_{z_1}logP(x_1,z_1|\theta) P(z_1|x_1,\theta^t) \sum_{z_2}P(z_2|x_2,\theta^t)\sum_{z_3}P(z_3|x_3,\theta^t)...\sum_{z_3}P(z_n|x_n,\theta^t) \tag{3}
$$



注意，在 (3) 式中，$z_1,z_2,z_3,...,z_n$这里是对数据进行积分，因为一共有 n 个 complete data，但是每一个 complete data 中一共有 k 种 Gaussian distribution **


$$
\sum_{z_2}P(z_2|x_2,\theta^t) = \sum_{z2_i}^KP(z2_i|x_2,\theta^t) = 1
$$

---

则(3) 式可以表示为


$$
\Longrightarrow \sum_{z_1}logP(x_1,z_1|\theta) P(z_1|x_1,\theta^t) \underbrace{\sum_{z_2}^KP(z_2|x_2,\theta^t)}_1 \underbrace{\sum_{z_3}P(z_3|x_3,\theta^t)}_1...\underbrace{\sum_{z_3}P(z_n|x_n,\theta^t) }_1\tag{4}
$$

$$
First\space term =\sum_{z_1}logP(x_1,z_1|\theta) P(z_1|x_1,\theta^t)  \tag{5}
$$



则 Q函数的完全形式可以表达为：



$$
Q(\theta,\theta^t) = \sum_i^N \sum_{z_i} logP(x_i,z_i|\theta)P(z_i|x_i,\theta^t) \tag{6}
$$



其中

$$
P(x_i,z_i) = P(z_i)P(x_i|z_i) = p_{z_i} N(\mu_i,\delta_i^2) \\
P(z_i|x_i) = \frac{P(x_i,z_i)}{P(x_i)} \\
P(x_i) = \sum_{z_i} p_{z_i} N(\mu_i,\delta_i^2)
$$


则(6) 可以表示为:



$$
Q(\theta,\theta^t) = \sum_i^N \sum_{z_i} p_{z_i} N(\mu_i,\delta_i^2)\frac{p_{z_i} N(\mu_i,\delta_i^2)}{\sum_{z_i} p_{z_i} N(\mu_i,\delta_i^2)}\tag{7}
$$




# Reference

1. [机器学习-白板推导系列（十一）-高斯混合模型GMM(Gaussian Mixture Model)](<https://www.bilibili.com/video/av35183585>)
2. [[EM算法及其应用（一）](https://www.cnblogs.com/massquantity/p/9248482.html)](<https://www.cnblogs.com/massquantity/p/9248482.html>)
3. [李航,统计学习方法](<http://www.dgt-factory.com/uploads/2018/07/0725/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95.pdf>)

