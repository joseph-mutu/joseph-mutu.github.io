---
layout:     post   				    # 使用的布局（不需要改）
title:      GMM 				# 标题 
subtitle:   GMM,MLE,EM #副标题
date:       2019-10-13 				# 时间
author:     WYX 						# 作者
header-img: img/1.2.png 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - PGM,EM,GMM,MLE
---

本文基本遵循“机器学习-白板推导系列（十一）-高斯混合模型GMM（Gaussian Mixture Model）”—shuahuai008（b站） 的推导

# GMM 高斯混合模型

### 离散变量边缘分布

从几何角度来看，样本采样的过程为多个高斯分布的叠加合成:


$$
p(x) = \sum_{k=1}^K \alpha_k N(x|\mu_k,\delta_k^2),\sum_{k=1}^K\alpha_k = 1
$$


![One dimension Gaussian model](https://pic.superbed.cn/item/5cb5d8fc3a213b04175a0779)



![Two-dimension Gaussian Model](https://pic.superbed.cn/item/5cb582273a213b041754f585)



从混合模型的角度来看

* X 为 observed data $\rightarrow$ X = {$x_1,x_2,...,x_n$}
* Z 为 latent variable
* $\sum_{k=1}^{K}p_k = 1$



对于GMM 存在下表，表示不同高斯分布的权重

|   Z   | $C_1$ | $C_2$ | $...$ | $C_N$ |
| :---: | :---: | :---: | :---: | :---: |
| $p_K$ | $p_1$ | $p_2$ | $...$ | $p_n$ |



某一个样本 $x$ ，它既有可能属于 $c_1$高斯分布，也有可能属于$c_2$高斯分布，但是属于$c_1$ 高斯分布的概率大一点



### 概率密度函数

一个样本的概率密度函数 $p(k)$可以表示为:



$$
p(x) = \sum_Z P(X,Z) \\
= \sum_{k=1}^K P(X,Z = C_k) \\
=  \sum_{k=1}^K P(Z = C_k) P(X|Z = C_k) \\
= \sum_{k=1}^K p_k N(x|\mu_k,\delta_k^2)
$$


## 生成过程

假设有一个 k 面的**不均匀骰子**,每一面出现的概率对应上表中的$p_k$

1. 投骰子，选择一个面，假设为 $p_k$
2. 选中为 $p_k$ 的高斯分布
3. 从$p_k$的高斯分布中采样，得到一个样本

 

## 为什么极大似然估计不行

对于一个样本，其参数为



* $\theta:$ {$p_1,p_2,...,p_k;\mu_1,\mu_2,...,\mu_k;\delta_1,\delta_2,...,\delta_n$}



极大似然过程可以表示为:



$$
\hat\theta_{MLE} = arg\max_{\theta} log\prod_i^Np(x_i) \\
 = arg\max_{\theta} \sum_i^N log p(x_i) \\
 = arg\max_{\theta} \sum_i^N log \space \underbrace {(\sum_{k=1}^K p_kN(x_i|\mu_k,\delta_k^2))}_{log(f_1+f_2+f_3+...+f_K)}
$$





其中单个高斯分布的概率密度函数可以表示为


$$
f_k = p_k N(x_i|\mu_k,\delta_k^2) = p_k \frac{1}{\sqrt{2\pi} \delta_k} exp(- \frac{(x_i-\mu_k)^2}{2\delta_k^2})
$$



上述过程无解析解



### 引入 EM 算法求解 GMM 模型(E-step)

* X: observed data $\rightarrow$X = {$x_1,x_2,...,x_n$}

* (X,Z): complete data $\rightarrow$$(x_1,z_1),(x_2,z_2),(x_3,z_3),...,(x_n,z_n)$



EM 算法一般形式表达


$$
\theta^{t+1} = arg\max_\theta \underbrace{ \mathbb{E}_{Z|X,\theta^t}[logP(X,Z|\theta)]}_{Q(\theta,\theta^t)}
$$

$$
\theta^{t+1} = arg\max_\theta Q(\theta,\theta^t)
$$



将 Q 函数进行展开:



$$
Q(\theta,\theta^t) = \int_Z P(Z|X,\theta^t) logP(X,Z|\theta) dz \\
= \sum_Z P(Z|X,\theta^t) logP(X,Z|\theta) \\
$$



在原本假设中，数据的采样为独立同分布$iid$



$$
= \sum_Z  log\prod_i^NP(x_i,z_i|\theta) \prod_i^N P(z_i|x_i,\theta^t) \\
= \sum_{z_1,z_2,...,z_n} \underbrace{\sum_i^N logP(x_i,z_i|\theta)}_{展开} \prod_i^N P(z_i|x_i,\theta^t)
$$

$$
=\sum_{z_1,z_2,...,z_n} [\underbrace{logP(x_1,z_1|\theta)}_{提取第一项}+logP(x_2,z_2|\theta)+...+logP(x_n,z_n|\theta)] \prod_i^N P(z_i|x_i,\theta^t)
$$

---



将第一项提取，进行分析


$$
\Longrightarrow \sum_{z_1,z_2,...,z_n}logP(x_1,z_1|\theta)\prod_i^N P(z_i|x_i,\theta^t)\\
\Longrightarrow \sum_{z_1,z_2,...,z_n}\underbrace{logP(x_1,z_1|\theta)}_{只与z_1 相关}\space[P(z_1|x_1,\theta^t) P(z_2|x_2,\theta^t)...P(z_n|x_n,\theta^t)]
$$


提取乘积的首项


$$
\Longrightarrow \sum_{z_1}logP(x_1,z_1|\theta) P(z_1|x_1,\theta^t) \underbrace{\sum_{z_2,z_3,...,z_n}[P(z_2|x_2,\theta^t)...P(z_n|x_n,\theta^t)]}_1 \tag{1}
$$

$$
\Longrightarrow \sum_{z_1}logP(x_1,z_1|\theta) P(z_1|x_1,\theta^t) \sum_{z_2,z_3,...,z_n}P(z_2|x_2,\theta^t) \sum_{z_3,z_3,...,z_n}P(z_3|x_3,\theta^t)...\sum_{z_2,z_3,...,z_n}P(z_n|x_n,\theta^t) \tag{2}
$$

$$
\Longrightarrow \sum_{z_1}logP(x_1,z_1|\theta) P(z_1|x_1,\theta^t) \sum_{z_2}P(z_2|x_2,\theta^t)\sum_{z_3}P(z_3|x_3,\theta^t)...\sum_{z_3}P(z_n|x_n,\theta^t) \tag{3}
$$



**注意，在 (3) 式中，$z_1,z_2,z_3,...,z_n$这里是对数据进行积分，因为一共有 n 个 complete data，但是每一个 complete data 中一共有 k 种 Gaussian distribution**



$$
\sum_{z_2}P(z_2|x_2,\theta^t) = \sum_{z2_i}^KP(z2_i|x_2,\theta^t) = 1
$$

---

则(3) 式可以表示为


$$
\Longrightarrow \sum_{z_1}logP(x_1,z_1|\theta) P(z_1|x_1,\theta^t) \underbrace{\sum_{z_2}^KP(z_2|x_2,\theta^t)}_1 \underbrace{\sum_{z_3}P(z_3|x_3,\theta^t)}_1...\underbrace{\sum_{z_3}P(z_n|x_n,\theta^t) }_1\tag{4}
$$

$$
First\space term =\sum_{z_1}logP(x_1,z_1|\theta) P(z_1|x_1,\theta^t)  \tag{5}
$$



则 Q函数的完全形式可以表达为：



$$
Q(\theta,\theta^t) = \sum_i^N \sum_{z_i} logP(x_i,z_i|\theta)P(z_i|x_i,\theta^t) \tag{6}
$$



其中

$$
P(x_i,z_i) = P(z_i)P(x_i|z_i) = p_{z_i} N(\mu_i,\delta_i^2) \\
P(z_i|x_i) = \frac{P(x_i,z_i)}{P(x_i)} \\
P(x_i) = \sum_{z_i} p_{z_i} N(\mu_i,\delta_i^2)
$$


则(6) 可以表示为:



$$
Q(\theta,\theta^t) = \sum_i^N \sum_{z_i} log [p_{z_i} N(x_i|\mu_i,\delta_i^2)]\underbrace{\frac{p_{z_i}^{(t)} N(x_i|\mu_i^{(t)},(\delta_i^2)^{(t)}) }{\sum_{z_i} p_{z_i}^{(t)} N(x_i|\mu_i^{(t)},(\delta_i^2)^{(t)})} }_{constant  }\tag{7}
$$



## 求解 GMM (M-step)

根据(7) 进一步化简 Q 函数


$$
Q(\theta,\theta^t) = \sum_i^N \sum_{k=1}^K [log(p_k) + logN(x_i|\mu_i,\delta_i^2)]\underbrace{P(z_k|x_i,\theta^t)}_{constant\space from \space E-step} (8)
$$


对于GMM模型整体来说，其参数个数为 3K 个，其中 K 为隐含的高斯分布的个数


$$
\theta_i = p_1,p_2,...,P_K;\mu_1,\mu_2,...,\mu_K;\delta_1,\delta_2,...,\delta_K
$$




### 求解p参数

$$
p^{(t+1)} = (p_1^{t+1},p_2^{t+1},...,p_K^{t+1})
$$



求解下述带约束的模型，
$$
p^{(t+1)} = arg\max_p \sum_i^N \sum_{k=1}^K log(p_k)P(z_k|x_i,\theta^t) \\ 
s.t \sum_{k=1}^K p_k = 1 \tag{9}
$$
构建拉格朗日方程，令导数等于 0 


$$
\cal{L}(\lambda,p) = \sum_i^N \sum_{k=1}^K log(p_k)P(z_k|x_i,\theta^t) + \lambda(\sum_{k=1}^K p_k - 1)
$$

下式为对K个p 中的一个 p 进行求解


$$
\frac{\partial \cal{L}(\lambda,p)}{\partial p_k} = \sum_i^N \frac{1}{p_k}P(z_k|x_i,\theta^t)  + \lambda \tag{10}
$$



令 （10） 导数为 0

$$
\Rightarrow \sum_i^N \frac{P(z_k|x_i,\theta^t)}{p_k}  + \lambda = 0 
$$

$$
\Rightarrow \sum_i^N P(z_k|x_i,\theta^t)  + p_k\lambda = 0 \tag{11}
$$


注意 (11) 式为一个通项公式，代表的仅为 K 个 p 参数中的 一个 p，一共有 K 个类似的方程，将 K 个方程进行叠加，可以得到下式：


$$
\Rightarrow \sum_{k=1}^K \sum_i^N P(z_k|x_i,\theta^t) + \lambda\underbrace{\sum_{k=1}^K p_k}_1 = 0 \tag{12}
$$

---

对 （12） 式子中的两个连加符号进行交换


$$
\Rightarrow \sum_i^N \underbrace{\sum_{k=1}^K P(z_k|x_i,\theta^t) }_1+ \lambda = 0 \tag{12}
$$


将 上式标为 1 的式子进行展开,查询(7)得到完整形式


$$
\sum_{k=1}^K\frac{p_{z_i}^{(t)} N(x_i|\mu_i^{(t)},(\delta_i^2)^{(t)}) }{\sum_{z_i} p_{z_i}^{(t)} N(x_i|\mu_i^{(t)},(\delta_i^2)^{(t)})} = 1
$$


**Note: 公式下方的 $\sum_{z_i}$也是从1到K的连加**

---



则 （12） 可以表示为


$$
\Rightarrow \sum_i^N 1 + \lambda = 0 \\
\Rightarrow \lambda = -N \tag{13}
$$


将 (13) 代入（11）可得: 


$$
\Rightarrow \sum_i^N P(z_k|x_i,\theta^t)  - Np_k  = 0 \\
\Rightarrow p_k = \frac{1}{N}\sum_i^N P(z_k|x_i,\theta^t) \tag{14}
$$





# Reference

1. [机器学习-白板推导系列（十一）-高斯混合模型GMM(Gaussian Mixture Model)](<https://www.bilibili.com/video/av35183585>)
