---
layout:     post   				    # 使用的布局（不需要改）
title:      Linear Gaussian Model				# 标题 
subtitle:   Bayesian Linear regression,multivariable gaussian           #副标题
date:       2019-10-16 				# 时间
author:     WYX 						# 作者
header-img: img/4.4.jpg 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - Bayesian Inference

---

---

# 线性高斯模型

假设存在数据 


$$
Data = \lbrace (x_1,y_1),(x_2,y_2)\ldots (x_N,y_N \rbrace \\
\space \\
x \in \mathbb{R}^P, y \in \mathbb{R} \\
\space \\
X = (x_1 ~  x_2 ~ \ldots ~ x_N )^T
$$


向量默认为`列向量`，则 $x$ 可以表示为矩阵的形式


$$
\left(
\begin{matrix}
x_{11} & x_{12} & x_{13} &\ldots & x_{1P} \\
x_{21} & x_{22} & x_{23} &\ldots & x_{2P} \\
\vdots & \vdots & \vdots & \vdots & \vdots \\
x_{N1} & x_{N2} & x_{N3} &\ldots & x_{NP}
\end{matrix}
\right)_{N \times P}
$$


设数据独立同分布，且服从于均值 $\mu$ 协方差矩阵为 $\Sigma$ 的高斯分布


$$
x_i \sim^{iid} N(\mu,\Sigma) \\
\space \\
\mu: P \times 1 ; \Sigma: P \times P \\
\space \\
\theta = (\mu ~\Sigma)
$$
其中多维高斯分布的公式如下：


$$
X \sim N(\mu,\Sigma) = \frac{1}{2\pi^{\frac{P}{2}} |\Sigma|^{\frac{1}{2}}} ~ exp \lbrace -\frac{1}{2}(x-\mu)^T \Sigma^{-1} (x-\mu) \rbrace
$$

### 重要定理

已知 $X$ 服从多维正态高斯分布 $N(\mu,\Sigma)$ ，若存在 $Y = AX + B$，则其期望与方差可以直接计算得出


$$
X \sim N(\mu,\Sigma) \\
\space \\

Y = AX +B \\
\space \\
\Rightarrow Y \sim N(A\mu + B,A\Sigma A^T)
$$
  

### 已知总体分布求边缘概率分布以及条件概率分布

已知数据 $x$ 服从多维高斯分布，则可以将 $x$ 其维度分为两块，如下


$$
x  = 
\left(
\begin{matrix}
x_a \\
x_b
\end{matrix}
\right),其中 x_a = m,x_b = n,m+n = P \\
\space \\
\mu  = 
\left(
\begin{matrix}
\mu_a \\
\mu_b
\end{matrix}
\right),
\Sigma  = 
\left(
\begin{matrix}
\Sigma_{aa} &\Sigma_{ab} \\
\Sigma_{ba} & \Sigma_{bb}
\end{matrix}
\right)
$$
则问题可以分为

- 求解 $p(x_a)$
- 求解 $p(x_b|x_a)$

#### 求$P(x_a)$

因为将 $x$ 分为两块，则可以将 $x_a$ 写为如下形式


$$
x_a = \underbrace{(I_m ~ 0)}_{A}\left( \begin{matrix}
x_a \\
x_b
\end{matrix}
\right)
$$


则根据定理可以写出 $x_a$ 的期望以及方差
$$
E[x_a] = (I_m ~ 0)\left( \begin{matrix}\mu_a \\\mu_b\end{matrix}\right) + 0 \\
= \mu_a \\
\space \\
Var[x_a] = A \Sigma A^T =  (I_m ~ 0)\left( \begin{matrix}\Sigma_{aa} & \Sigma_{ab} \\\Sigma_{ba} & \Sigma_{bb}\end{matrix}\right)\left( \begin{matrix}I_m \\ 0\end{matrix}\right) = \Sigma_{aa}
$$


#### 求 $P(x_b|x_a)$

条件概率的求解依据`构造性证明` ，上述条件概率的解释就是，给定 $x_a$ 的情况下，$x_b$ 的概率

先构造出新变量 $x_{b\cdot a}$ 如下


$$
x_{b\cdot a} = x_b - \Sigma_{ba}\Sigma_{aa}^{-1}x_a \\
\space \\
~~~结论： \\
\mu_{b\cdot a} = \mu_b-\mu_a \Sigma_{ba}\Sigma_{aa}^{-1} \\
\space \\
\Sigma_{bb\cdot a} = \Sigma_{bb}-\Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab}
$$


对 $x_{b\cdot a}$ 重写如下
$$
x_{b\cdot a} = \underbrace{(-\Sigma_{ba}\Sigma_{aa}^{-1} ~~~I_n)}_{A}\left( \begin{matrix}x_a \\ x_b\end{matrix}\right)
$$


则可以写出 $x_{b\cdot a} $ 的期望


$$
E[x_{b\cdot a} ] = (-\Sigma_{ba}\Sigma_{aa}^{-1} ~~~I_n) \left( \begin{matrix}\mu_a \\ \mu_b\end{matrix}\right) = \mu_b-\mu_a \Sigma_{ba}\Sigma_{aa}^{-1}
$$
方差如下，注意其中 $\Sigma_{aa}$ 为对称矩阵，所以其转置等于自身


$$
Var[x_{b\cdot a}] = A \Sigma A^T = (-\Sigma_{ba}\Sigma_{aa}^{-1} ~~~I_n) \left( \begin{matrix}\Sigma_{aa} & \Sigma_{ab} \\\Sigma_{ba} & \Sigma_{bb}\end{matrix}\right) \left( \begin{matrix}-\Sigma_{aa}^{-1}\Sigma_{ba}^T\\ I_n\end{matrix}\right)\\
= \left( \begin{matrix}-\Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{aa} + \Sigma_{ba}\\ -\Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab} + \Sigma_{bb}\end{matrix}\right)\left( \begin{matrix}-\Sigma_{aa}^{-1}\Sigma_{ba}^T\\ I_n\end{matrix}\right) \\
= \left( \begin{matrix}0\\ \Sigma_{bb}-\Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab}\end{matrix}\right)\left( \begin{matrix}-\Sigma_{aa}^{-1}\Sigma_{ba}^T\\ I_n\end{matrix}\right) \\
\space \\
= \Sigma_{bb}-\Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab}
$$


由此，可知 $x_{b\cdot a}$ 服从一个多元高斯分布，如下


$$
x_{b\cdot a} \sim N(\mu_{b \cdot a},\Sigma_{bb\cdot a})
$$


根据 $x_{b\cdot a}$，可以推出 $x_b$ 与 $x_a$ 之间存在关系式如下


$$
x_{b\cdot a} = x_b - \Sigma_{ba}\Sigma_{aa}^{-1}x_a  \\
\space \\
\Rightarrow  x_b = x_{b\cdot a}- \underbrace{\Sigma_{ba}\Sigma_{aa}^{-1}x_a}_{Constant}， 在求 p(x_b|x_a) 中，x_a 已经给定
$$


将上式 $x_b$ 与 $x_a$ 的关系式带入定理，可得


$$
E[x_b|x_a] = \underbrace{A}_{I} E[x_{b\cdot a}] + B = \mu_{b\cdot a}+ \Sigma_{ba}\Sigma_{aa}^{-1}x_a \\
\space \\
Var[x_b|x_a] = \Sigma_{bb\cdot a}
$$




## 有偏估计与无偏估计

假设在 **2 维**的情况下，对 $\theta$ 进行 MLE 估计，得到 $\theta_{MLE}$ 如下


$$
\mu_{MLE} = \frac{1}{N} \sum _{i=1} ^N x_i \\\delta_{MLE}^2 = \frac{1}{N} \sum _{i=1} ^N (x_i - \mu_{MLE})^2
$$
$\delta_{MLE}$ 的估计值为**有偏估计**，对$\delta_{MLE}$ 求期望如下
$$
E[\delta_{MLE}] = E[\underbrace{\frac{1}{N} \sum _{i=1} ^N (x_i - \mu_{MLE})^2}] \tag{1}
$$
对上述括号内的式子，提出来并进行简化


$$
\frac{1}{N} \sum _{i=1} ^N (x_i - \mu_{MLE})^2 = \frac{1}{N} \sum _{i=1} (x_i^2 - 2x_i\mu_{MLE} + \mu_{MLE}^2) \\\space 将 x_i 与 \mu_{MLE} 的关系带入 \\= \frac{1}{N} \sum _{i=1} ^N x_i^2 - 2\mu_{MLE}\sum _{i=1} ^N x_i + N\mu_{MLE}^2 \\= \frac{1}{N} \sum _{i=1} ^N x_i^2 - 2\mu_{MLE} \times N\mu_{MLE} + N\mu_{MLE}^2 \\= \frac{1}{N} \sum _{i=1} ^N x_i^2 - 2N\mu_{MLE}^2 + N\mu_{MLE}^2 \\= \frac{1}{N} \sum _{i=1} ^N x_i^2 - N\mu_{MLE}^2 \\= \frac{1}{N} \sum _{i=1} ^N x_i^2 - \mu_{MLE}^2\tag{2}
$$
重写 (1) 式可以得到
$$
E[\delta_{MLE}] =  E[\frac{1}{N}\sum _{i=1} ^N x_i^2 - \mu_{MLE}^2 \tag{2}] \\\space 添加一个 \mu^2 减去一个 \mu^2\\=   E[\frac{1}{N}(\sum _{i=1} ^N x_i^2 - \mu^2) - (\mu_{MLE}^2 - \mu^2)] \\= E[\frac{1}{N} \sum _{i=1} ^N (x_i^2 - \mu^2)] -  E[ (\mu_{MLE}^2 - \mu^2)] \\= \frac{1}{N} \sum _{i=1} ^NE[x_i^2 - \mu^2] - (E[\mu_{MLE}^2] - E[\mu^2)]) \\= \frac{1}{N} N Var(x_i) - (E[\mu_{MLE}^2] - \underbrace{\mu^2}_{E[\mu_{MLE}]^2}) \\= Var(x_i)  - Var(\mu_{MLE}) \\= \delta^2 - Var[ \frac{1}{N} \sum _{i=1} ^N x_i] = \delta^2 -  \frac{1}{N^2} \sum _{i=1} ^N Var(x_i) \\= \delta^2 - \frac{1}{N^2}N \delta^2 = \delta^2 - \frac{1}{N} \delta^2 = \frac{N-1}{N}\delta^2
$$
从而可知，通过 MLE 对 $\delta^2$ 的估计为有偏估计，所以需要消除其有偏性，也就是使得


$$
\frac{N}{N-1}E[\delta_{MLE}^2] =\frac{N}{N-1} E[\frac{1}{N} \sum _{i=1} ^N (x_i - \mu_{MLE})^2] \\= \frac{1}{N-1}E[ \sum _{i=1} ^N (x_i - \mu_{MLE})^2]
$$
